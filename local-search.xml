<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>6.824</title>
    <link href="/2022/09/29/6-824/"/>
    <url>/2022/09/29/6-824/</url>
    
    <content type="html"><![CDATA[<p>一个多月没写博客了。是因为入了大名鼎鼎的巨坑——6.824分布式系统的lab。整整一个月爆肝，现在终于写完了，开心。目前Lab4-ShardKv测试了5000次，但也不保证bug free。</p><details>    <summary>我个人的时间分布</summary><!-- empty line --><p>（不太准确，中间有的时间可能摸鱼了，比如中秋节）</p><ul><li>Lab1-MapReduce：2天左右（8月28通过）</li><li>Lab2-Raft：共16天<ul><li>2A：3天左右（8月31通过）</li><li>2B：5天左右（9月5通过）</li><li>2C：2天左右（9月7通过）</li><li>2D：6天左右（9月13通过）</li></ul></li><li>Lab3-KVRaft：共3天<ul><li>3A：1天左右（9月14通过）</li><li>3B：2天左右（9月16通过）</li></ul></li><li>Lab4-ShardKV：共12天<ul><li>4A：3天左右（9月19通过）</li><li>4B：5天左右（9月25通过）<ul><li>Challenge1+2：4天左右（9月29通过）</details><!-- empty line --></li></ul></li></ul></li></ul><p>结合个人感受以及时间分布，我认为Lab4B和Lab2D最难。Lab2具体做法都有参照，而Lab4则比较“自由发挥”。大部分时间都在盯着屏幕阅读log进行“happy” debug，而修bug是非常琐碎的过程，无法形成系统的知识，就不往博客里面写了。这篇博客我主要还是想讲一讲我Lab4B的做法。我做的是<a href="https://pdos.csail.mit.edu/6.824/labs/lab-shard.html">2022年的Lab</a>。根据<a href="https://pdos.csail.mit.edu/6.824/general.html">Collaboration Policy</a>，代码我就不公开了。</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Lab3是将数据分布到不同机器上，增加了Fault Tolerance，即便有一部分机器挂了，也能继续运行。但是对不同Key的操作要通过相同的Leader进行Start，直到commit。因此速度与单机相比没有提升（甚至由于存在commit操作，反而更慢了）</p><p>而Lab4在Lab3的基础上，将Key分散到各个ReplicaGroup（RG）上。不同RG管理不同的Shards，让不同Key的请求可以同时进行，从而提高performance。</p><p>先介绍一些概念：</p><ul><li>Shard（S）：代表一个key-value pair子集。比如‘a’开头的key都在同一个Shard中。（在Lab中，Shards数量是10）</li><li>Replica Group（RG或G）：负责一部分Shards（比如前三个Shards）的机器集群，每个都对应一个Raft集群来保证一致性。</li><li>Shard Controller（SC）：负责分配RG与Shard的对应关系<code>configuration</code>（Config或C）。这个关系会随时间变化。SC集群只有一个，通过raft支撑一致性。</li><li>Client请求的流程：通过SC得知key对应的Shard对应的RG，然后去问RG。</li></ul><p>Shards需要在不同的RG之间进行多次迁移。其目的：workload balance，或者因为存在新增或删除RG的操作。</p><p>因此，<strong>挑战</strong>是在迁移的过程中，依旧要保持数据一致性。推荐将reconfiguration与数据操作Op同时加入到raft的log中。</p><p>我们忽略Raft层面的细节，并且不用管Lab4A：Shard Controller（SC）。</p><h1 id="思考过程"><a href="#思考过程" class="headerlink" title="思考过程"></a>思考过程</h1><p>首先我们需要弄清楚机器有哪些操作。</p><h2 id="RG为单机"><a href="#RG为单机" class="headerlink" title="RG为单机"></a>RG为单机</h2><p>先将RG简化为单机（不考虑Raft），我们应该如何实现它的各种操作？</p><p>假设我们Data保存了所有Shard的数据，也保存了历史中经过的所有Config的Data。</p><ul><li>【收到Get，Put，Append】数据操作：根据自己的Config，检查key对应的Shard是否正确，然后进行正常操作就行</li><li>定时去SC那查询最新的Config，如果查询到比自己更新的Config<ol><li>更新自己的Config</li><li>比较负责的Shard的变更：<ul><li>如果之前没有负责S，现在需要负责S，则需要找到之前负责S的RG，发送GetShard请求【发送GetShard】。在收到GetShard回复之前，都不能处理其对应S的请求。收到GetShard回复之后，更新自己的数据。自己拿到数据之后，让之前负责的RG删除数据【发送DeleteBefore】。</li><li>如果之前负责S，现在不需要负责S了。等待其负责的RG找我来GetShard，在回复之前都可以继续负责S。</li></ul></li></ol></li><li>【收到GetShard】如果收到RG发过来的GetShard请求，说明自己一定在那个Config中在负责此Shard<ul><li>我有请求的Shard，则返回，并且停止处理S。</li><li>有可能我现在也没有拿到Shard，但是我一定会拿到，所以让其等待一段时间后重试</li></ul></li><li>【收到DeleteBefore】删除之前的数据。</li></ul><h2 id="RG为多机"><a href="#RG为多机" class="headerlink" title="RG为多机"></a>RG为多机</h2><p>多机在单机的基础上，添加：Data与Config的更新操作需要让所有机器执行。与单机不同的地方用#表明。</p><ul><li>【收到Get，Put，Append】数据操作：根据自己的Config，检查key对应的Shard是否正确，然后进行正常操作就行</li><li>定时去SC那查询最新的Config，如果查询到比自己更新的Config<ol><li>####【发送UpdateConfig】####，从而更新自己的Config</li><li>比较负责的Shard的变更：<ul><li>如果之前没有负责S，现在需要负责S，则需要找到之前负责S的RG，发送GetShard请求【发送GetShard】。在收到GetShard回复之前，都不能处理其对应S的请求。收到GetShard回复之后，更新自己的数据，####【发送UpdateData】####。自己拿到数据之后，让之前负责的RG删除数据【发送DeleteBefore】。</li><li>如果之前负责S，现在不需要负责S了。等待其负责的RG找我来GetShard，在回复之前都可以继续负责S。</li></ul></li></ol></li><li>【收到GetShard】如果收到RG发过来的GetShard请求，说明自己一定在那个Config中在负责此Shard<ul><li>我有请求的Shard，则返回，并且停止处理S。</li><li>有可能我也没有拿到Shard，但是我一定会拿到，所以让其等待一段时间后重试</li></ul></li><li>####【收到UpdateConfig】####，更新自己的Config。</li><li>####【收到UpdateData】####，更新自己的Data。</li><li>【收到DeleteBefore】删除之前的数据。</li></ul><h2 id="操作如何执行"><a href="#操作如何执行" class="headerlink" title="操作如何执行"></a>操作如何执行</h2><p>接下来我们需要弄清楚这些操作要在什么时候执行，由谁来执行。</p><ol><li><p>【收到Put，Append，UpdateConfig，UpdateData，DeleteBefore】以及【收到GetShard：修改Config来停止处理对某些Shard的请求】。这些属于RG的状态变化，需要所有机器执行。通过Raft的applyCh来给所有Server发，在每个Server apply命令的时候执行。</p></li><li><p>【收到Get，GetShard】。这些属于读取RG的状态。需要让收到请求的机器在得知命令commit之后，自己执行。需要用Raft来保证Linearizable。对于GetShard来说，在apply的时候需要额外修改config来停止对某些Shard的请求。</p></li><li><p>【发送UpdateConfig，发送GetShard，发送UpdateData，发送DeleteBefore】。属于自己作为客户端来发送请求。</p><p>他们只需要其中一个机器去发送请求，因此不属于1类，并且执行过程中可能block，不能让他们影响正常的apply过程（容易死锁）。这些操作需要自己来触发，没有外部请求来触发，因此也不属于2类。所以，最简单的做法是将其作为两个单独的定时任务：</p><ol><li><p>（定时）找SC拿最新Config，如果需要更新，触发【发送UpdateConfig】</p></li><li><p>（定时）从旧往新遍历所有Data中所有历史Config，所有Shards，看是否存在需要【发送GetShard】，如果收到正确回复，再【发送UpdateData】与【发送DeleteBefore】</p></li></ol><p>我觉得这么做没问题，但是我实际没有实现ii，因为Data在大部分时间都不需要GetShard，只有在Config更新之后才需要，如果作为定时任务，可能比较耗时。</p><p>我的做法是：在UpdateConfig命令Apply的时候，让Leader去异步执行ii。由于Log中记录了UpdateConfig命令，因此，每次Backup的时候也会重新执行ii。</p></li></ol><details>    <summary>为什么不能将这两个简单合并为一个定时任务</summary><!-- empty line --><blockquote><p>可能有问题的做法：定时去SC那拿最新Config，如果要更新，就触发【发送UpdateConfig】，然后遍历所有Data中所有Shard所有历史Config，看是否存在需要【发送GetShard】，如果收到正确回复，再【发送UpdateData】与【发送DeleteBefore】</p></blockquote><p>如果【发送UpdateConfig】成功了。Config更新之后被Killed了，没来得及完成【发送GetShard】。假设之后没有新Config，于是之后就不会再触发【发送GetShard】了。这块Shard一直无法拿到，也就无法处理数据请求。</p><hr>  </details>  <!-- empty line --><h2 id="如何找到之前负责S的RG"><a href="#如何找到之前负责S的RG" class="headerlink" title="如何找到之前负责S的RG"></a>如何找到之前负责S的RG</h2><p>RG拿到新的Config，发现需要负责某个S，怎么找到之前负责该S的RG呢？注意Config的更新之间可能会跳过一些Config。我尝试了两种做法。</p><h3 id="一级级往前问，找到谁拿着最新的S"><a href="#一级级往前问，找到谁拿着最新的S" class="headerlink" title="一级级往前问，找到谁拿着最新的S"></a>一级级往前问，找到谁拿着最新的S</h3><p>（通过1280次除Challenge1之外的4B，但是大概率还有bug，只是提供另一种思路，并且这种方式我不知道怎么实现Delete历史数据，所以Challenge1过不了）</p><ol><li>发送GetShard：UpdateConfig后，查询之前的Config，给之前可能会负责的RG发送GetShard，如果回复<code>ErrNoResponsibility</code>则继续向上一级询问。</li><li>收到GetShard：<ul><li>如果Request的Config大于当前我的Config，说明我还没有负责，停止对S的操作（直到我的Config大于Request的C），返回<code>ErrNoResponsibility</code>。</li><li>如果Request正好是当前的Config，则停止对S的操作，并返回当前的Data。</li><li>如果Request小于当前的Config，则查询历史Data，如果有则返回，否则<code>ErrNoResponsibility</code>（这里可能还有问题）。</li></ul></li></ol><p>举个例子：假设S的负责过程如下，假设我们是G3，拿到最新的C4，而最新的S在C1的G1手中</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs go">C0: <span class="hljs-number">0</span><br>C1: G1 <span class="hljs-comment">// S</span><br>C2: G2 <span class="hljs-comment">// -&gt; ErrNoResponsibility</span><br>C3: G1 <span class="hljs-comment">// -&gt; ErrNoResponsibility</span><br>C4: G3 <span class="hljs-comment">// G3</span><br></code></pre></td></tr></table></figure><p>首先G3向上一级C3的G1问是否有S，G1回复不负责，G1停止处理S的数据请求（直到收到比C4的G3更高的Config）。由于没问到，G3再向上一级C2的G2问，G2同意不负责，G2停止处理S的数据请求。G3再向上一级C1的G1问，负责，并返回此时的S。</p><h3 id="保证每个C中负责S的RG都要拿到S（Step-By-Step）"><a href="#保证每个C中负责S的RG都要拿到S（Step-By-Step）" class="headerlink" title="保证每个C中负责S的RG都要拿到S（Step-By-Step）"></a>保证每个C中负责S的RG都要拿到S（Step-By-Step）</h3><p>通过5000次4B包括Challenge，感觉这个实现起来更容易一点。思路：</p><ol><li>发送GetShard：UpdateConfig后，<strong>只</strong>查询上一级的Config，并给对应RG其发送GetShard，直到GetShard成功返回。</li><li>收到GetShard：如果有数据，则返回，否则让其等待后重试（避免互相发导致死锁）。</li></ol><h1 id="调试中的琐碎细节"><a href="#调试中的琐碎细节" class="headerlink" title="调试中的琐碎细节"></a>调试中的琐碎细节</h1><h2 id="Reconfiguration"><a href="#Reconfiguration" class="headerlink" title="Reconfiguration"></a>Reconfiguration</h2><ul><li>历史Config不会变化，因此可以给Configs缓存，减少与SC之间的RPC数量</li><li>Client调用需要server名字，但要更新的Config中不一定包含该RG，因此需要查询历史Config去寻找</li><li>GetShard的重试时间可以设置为随机时间减少互相发GetShard导致死锁的可能</li></ul><h2 id="Applier"><a href="#Applier" class="headerlink" title="Applier"></a>Applier</h2><ul><li>commitIndex需要包括所有apply的命令，包括那些重复的，WrongGroup的命令。不然可能出现Snapshot不减少Log长度的情况。</li><li>GetShard在apply的时候，停止后续对某些Shard的处理，仅将config.Shard改为-1还不够。需要提示出直到什么configNum才能继续处理。</li><li>Step-By-Step的做法带来的性质：如果C中的S全部完成交接，则C之前的S也一定全部完成了。因此在“从旧往新遍历所有Data中所有历史Config”之前，可以先从新往旧遍历，找到交接完成的最后一个C，从那之后进行GetShard。</li><li>去重相关的map也需要随data一样，切分为不同的Shard，同时也需要在GetShard中进行传输</li></ul><h2 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h2><ul><li><p>不同的RG各自需要一套request ID来去重。</p></li><li><p>在Challenge2中，要求Config更新后，集群需要在1秒内完成所有GetShard的操作，因此我们可以降低集群内RPC的ErrLeader重试时间（本来是100毫秒，可能几次重试就超时了）。</p></li><li><p>需要注意不论是SC的Client还是ShardKV的Client，都存在并发请求的时候（SC的Query，ShardKV的内部请求）。不要直接锁住整个RPC调用。</p></li></ul><h1 id="Bonus-Shard-Controller如何最小化Shard移动"><a href="#Bonus-Shard-Controller如何最小化Shard移动" class="headerlink" title="Bonus: Shard Controller如何最小化Shard移动"></a>Bonus: Shard Controller如何最小化Shard移动</h1><p>我觉得这个也值得一提</p><h3 id="计算每个G负责的S数量"><a href="#计算每个G负责的S数量" class="headerlink" title="计算每个G负责的S数量"></a>计算每个G负责的S数量</h3><p>$N$为Shards的数量，$G_i$为第$i$个RG分配的S数量。$NG$为RG的数量。</p><p>则$G_i &#x3D; N &#x2F; NG$，$N$ -&#x3D; $G_i$，$NG$ -&#x3D; 1。其中&#x2F;为整除。</p><h3 id="如何分配"><a href="#如何分配" class="headerlink" title="如何分配"></a>如何分配</h3><p>遍历每个Shard：</p><ol><li>如果old config中的G还有可持有数量不为0，则继续让其作为持有者，并将其可持有数量减一。</li><li>如果old config中的G可持有数量为0，则先跳过。</li></ol><p>最后，如果存在S没有G持有，则将其给剩余有可持有数量的G，这里的顺序无所谓，因为要么就是Joined RG来拿RG，要么就是RG来拿Leaved RG。</p><p>看了下面这个例子就清楚了。假设Shard数量为10，我们考虑多个G加入或离开时，各自应该持有的S：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs go">C0: <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-comment">// Join 0, Gi = [10]</span><br><span class="hljs-comment">// Join 1, Gi = [5, 5]</span><br>C1: <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span><br><span class="hljs-comment">// Join 2, Gi = [3, 3, 4]</span><br>Cm: <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> x x <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> x x<br>C2: <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span><br><span class="hljs-comment">// Join 3, Gi = [2, 2, 3, 3]</span><br>Cm: <span class="hljs-number">0</span> <span class="hljs-number">0</span> x <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> x <span class="hljs-number">2</span> x<br>C3: <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span><br><span class="hljs-comment">// Leave 3, Gi = [3, 3, 4]</span><br>Cm: <span class="hljs-number">0</span> <span class="hljs-number">0</span> x <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> x <span class="hljs-number">2</span> x <span class="hljs-comment">// Gi = [1, 1, 1]</span><br>C4: <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span><br><span class="hljs-comment">// Leave 1, Gi = &#123;0:5, 2:5&#125;</span><br>Cm: <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> x x x <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-comment">// Gi = &#123;0:2, 2:1&#125;</span><br>C4: <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Golang中的channel与sync.Mutex</title>
    <link href="/2022/08/30/Golang%E4%B8%AD%E7%9A%84channel%E4%B8%8Esync-Mutex/"/>
    <url>/2022/08/30/Golang%E4%B8%AD%E7%9A%84channel%E4%B8%8Esync-Mutex/</url>
    
    <content type="html"><![CDATA[<h2 id="一个现象"><a href="#一个现象" class="headerlink" title="一个现象"></a>一个现象</h2><p>假设我们有一个任务，包括两个goroutine：</p><ol><li>writer：负责每秒给counter加一</li><li>reader：负责每秒打印counter的值</li></ol><p>如果用lock的话</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> Controller <span class="hljs-keyword">struct</span> &#123;<br>cnt <span class="hljs-type">int</span><br>mu sync.Mutex<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Controller)</span></span> writer () &#123;<br><span class="hljs-keyword">for</span> &#123;<br>c.mu.Lock()<br>c.cnt += <span class="hljs-number">1</span><br>c.mu.Unlock()<br>time.Sleep(time.Second)<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Controller)</span></span> reader () &#123;<br><span class="hljs-keyword">for</span> &#123;<br>c.mu.Lock()<br>fmt.Printf(<span class="hljs-string">&quot;read: %v\n&quot;</span>, c.cnt)<br>c.mu.Unlock()<br>time.Sleep(time.Second)<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>  &#123;<br>c := Controller&#123;&#125;<br><span class="hljs-keyword">go</span> c.writer()<br><span class="hljs-keyword">go</span> c.reader()<br>time.Sleep(<span class="hljs-number">20</span> * time.Second)<br>&#125;<br></code></pre></td></tr></table></figure><p>【这是错误的】：一开始我以为是reader和writer交替，假设writer先抢到lock，那么reader一定在lock那里block，然后等writer做完操作，释放lock并进入sleep时，reader被唤醒，进行读操作。怎么可能同时发生两次write呢？</p><p><img src="/img/8.png" alt="Untitled"></p><p>但是运行后，发现读写（居然）并非交替运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt; go run lock.go<br><span class="hljs-built_in">read</span>: 1<br><span class="hljs-built_in">read</span>: 1<br><span class="hljs-built_in">read</span>: 3<br><span class="hljs-built_in">read</span>: 3<br><span class="hljs-built_in">read</span>: 5<br>...<br></code></pre></td></tr></table></figure><p>原因是临界区中操作时间过短，导致两个goroutine接近同时操作完后同时进入等待，接近同时唤醒，接近同时抢lock，图中两个虚线之间则是两个goroutine同时抢lock的区间</p><p><img src="/img/10.png" alt="Untitled"></p><p>如果给reader和writer的操作都加一点时间，则可以顺序打印，如writer改为：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Controller)</span></span> writer () &#123;<br><span class="hljs-keyword">for</span> &#123;<br>c.mu.Lock()<br>c.cnt += <span class="hljs-number">1</span><br>time.Sleep(time.Second)<br>c.mu.Unlock()<br>time.Sleep(time.Second)<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>运行验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">&gt; go run lock.go<br><span class="hljs-built_in">read</span>: 1<br><span class="hljs-built_in">read</span>: 2<br><span class="hljs-built_in">read</span>: 3<br><span class="hljs-built_in">read</span>: 4<br><span class="hljs-built_in">read</span>: 5<br>...<br></code></pre></td></tr></table></figure><h2 id="用channel代替mutex"><a href="#用channel代替mutex" class="headerlink" title="用channel代替mutex"></a>用channel代替mutex</h2><p>我们如何使用channel来代替以上的mutex，并实现相同的效果？</p><p>先分析Lock和Unlock操作：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs go">c.mu.Lock()<br>c.cnt += <span class="hljs-number">1</span><br>c.mu.Unlock()<br></code></pre></td></tr></table></figure><p>Lock操作是block类型，也即如果没有Mutex，需要一直等待；而Unlock不是block类型，不管有没有goroutine正在Lock这里被block了，反正Unlock都能执行完。</p><p>因此，不能简单的用unbuffered channel进行替换，【错误的】例如：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> void <span class="hljs-keyword">struct</span>&#123;&#125;<br><span class="hljs-keyword">type</span> Controller <span class="hljs-keyword">struct</span> &#123;<br>cnt <span class="hljs-type">int</span><br>ch  <span class="hljs-keyword">chan</span> void<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Controller)</span></span> writer() &#123;<br><span class="hljs-keyword">for</span> &#123;<br><span class="hljs-comment">// block here</span><br>&lt;-c.ch<br>c.cnt += <span class="hljs-number">1</span><br><span class="hljs-comment">// also block here (different from mutex)</span><br>c.ch &lt;- void&#123;&#125;<br>time.Sleep(time.Second)<br>&#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Controller)</span></span> reader() &#123;<br><span class="hljs-keyword">for</span> &#123;<br><span class="hljs-comment">// block here</span><br>&lt;-c.ch<br>fmt.Printf(<span class="hljs-string">&quot;read: %v\n&quot;</span>, c.cnt)<br><span class="hljs-comment">// also block here (different from mutex)</span><br>c.ch &lt;- void&#123;&#125;<br>time.Sleep(time.Second)<br>&#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br>c := Controller&#123;<br>ch:     <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> void),<br>&#125;<br><span class="hljs-keyword">go</span> c.writer()<br><span class="hljs-keyword">go</span> c.reader()<br>c.ch &lt;- void&#123;&#125;<br>time.Sleep(<span class="hljs-number">20</span> * time.Second)<br>&#125;<br></code></pre></td></tr></table></figure><p><img src="/img/9.png" alt="Untitled"></p><p>所以运行结果是差不多每两秒打印一个值，并且是顺序打印。</p><p>因此我们需要让Unlock对应的操作不属于block模式</p><h3 id="buffer-x3D-1的channel"><a href="#buffer-x3D-1的channel" class="headerlink" title="buffer&#x3D;1的channel"></a>buffer&#x3D;1的channel</h3><p>使用一个buffer&#x3D;1的channel来代替mutex，则会让放数据到channel的操作block，而拿数据则由于发生同一个goroutine中的放数据之后，因此能保证channel中有数据，因此不会block。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> void <span class="hljs-keyword">struct</span>&#123;&#125;<br><br><span class="hljs-keyword">type</span> Controller <span class="hljs-keyword">struct</span> &#123;<br>cnt <span class="hljs-type">int</span><br>ch  <span class="hljs-keyword">chan</span> void<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Controller)</span></span> writer() &#123;<br><span class="hljs-keyword">for</span> &#123;<br>c.ch &lt;- void&#123;&#125;<br>c.cnt += <span class="hljs-number">1</span><br>&lt;-c.ch<br>time.Sleep(time.Second)<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Controller)</span></span> reader() &#123;<br><span class="hljs-keyword">for</span> &#123;<br>c.ch &lt;- void&#123;&#125;<br>fmt.Printf(<span class="hljs-string">&quot;read: %v\n&quot;</span>, c.cnt)<br>&lt;-c.ch<br>time.Sleep(time.Second)<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br>c := Controller&#123;<br>ch:     <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> void, <span class="hljs-number">1</span>),<br>&#125;<br><span class="hljs-keyword">go</span> c.writer()<br><span class="hljs-keyword">go</span> c.reader()<br>time.Sleep(<span class="hljs-number">5</span> * time.Second)<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="new-goroutine解决block的问题"><a href="#new-goroutine解决block的问题" class="headerlink" title="new goroutine解决block的问题"></a>new goroutine解决block的问题</h3><p>将unbuffered channel那种做法中的放数据到channel那步放到新建的goroutine中来做：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> void <span class="hljs-keyword">struct</span>&#123;&#125;<br><br><span class="hljs-keyword">type</span> Controller <span class="hljs-keyword">struct</span> &#123;<br>cnt <span class="hljs-type">int</span><br>ch  <span class="hljs-keyword">chan</span> void<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Controller)</span></span> writer() &#123;<br><span class="hljs-keyword">for</span> &#123;<br>&lt;-c.ch<br>c.cnt += <span class="hljs-number">1</span><br><span class="hljs-comment">// without block</span><br><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123; c.ch &lt;- void&#123;&#125; &#125;()<br>time.Sleep(time.Second)<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Controller)</span></span> reader() &#123;<br><span class="hljs-keyword">for</span> &#123;<br>&lt;-c.ch<br>fmt.Printf(<span class="hljs-string">&quot;read: %v\n&quot;</span>, c.cnt)<br><span class="hljs-comment">// without block</span><br><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123; c.ch &lt;- void&#123;&#125; &#125;()<br>time.Sleep(time.Second)<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br>c := Controller&#123;<br>ch: <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> void),<br>&#125;<br><span class="hljs-keyword">go</span> c.writer()<br><span class="hljs-keyword">go</span> c.reader()<br>c.ch &lt;- void&#123;&#125;<br>time.Sleep(<span class="hljs-number">20</span> * time.Second)<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>利用Scheduling Framework实现一个简单的gang调度器</title>
    <link href="/2022/08/14/%E5%88%A9%E7%94%A8Scheduling-Framework%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84gang%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    <url>/2022/08/14/%E5%88%A9%E7%94%A8Scheduling-Framework%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84gang%E8%B0%83%E5%BA%A6%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<p>写在前面，本项目<a href="https://github.com/FFFFFaraway/gang-scheduler">gang-scheduler</a>面向<a href="https://github.com/kubernetes-sigs/scheduler-plugins">此项目</a>中的cosheduler造轮子，如果只是想使用gang调度器，推荐阅读原项目；如果想了解如何自定义调度器，可以看看本项目，其改用ConfigMap而没有用CRD来管理PodGroup，也省略了一些优化，是原项目的简陋版，仅方便入门学习。</p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>在上一篇文章中，我们写了一个简单的MPI Operator，其可以根据MPIJob任务需求生成Pod来进行分布式训练。在使用过程中，我发现一个不是很好的现象：在资源不足的情况下，新建一个训练任务，虽然资源（例如GPU）不足以满足训练任务，但集群依旧会“尽量”（能分配一个Pod就分配一个Pod）分配资源生成Worker，使一部分Worker处于Running状态，等待Launcher的命令，而另一部分Worker则由于后续资源不足而处于Pending的状态，导致任务无法启动，而浪费了资源。</p><p>比如，假设我们需要5个GPU来进行训练任务，而集群中可用的GPU只有4个。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">batch.test.bdap.com/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">MPIJob</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">simple-train</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">sw-mpi-operator</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">numWorkers:</span> <span class="hljs-number">5</span><br>  <span class="hljs-attr">launcherTemplate:</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">containers:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">farawaya/horovod-torch-cpu</span><br>          <span class="hljs-attr">name:</span> <span class="hljs-string">horovod-master</span><br>  <span class="hljs-attr">workerTemplate:</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">containers:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">args:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">sleep</span> <span class="hljs-string">infinity</span><br>          <span class="hljs-attr">command:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/sh</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">-c</span><br>          <span class="hljs-attr">image:</span> <span class="hljs-string">farawaya/horovod-torch-cuda113</span><br>          <span class="hljs-attr">name:</span> <span class="hljs-string">horovod-worker</span><br>          <span class="hljs-attr">resources:</span><br>            <span class="hljs-attr">limits:</span><br>              <span class="hljs-attr">nvidia.com/gpu:</span> <span class="hljs-number">1</span><br>      <span class="hljs-attr">tolerations:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span><br>          <span class="hljs-attr">key:</span> <span class="hljs-string">gpu</span><br>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span><br></code></pre></td></tr></table></figure><p>根据MPI Operator中的流程，由于Workers未全部就绪，无法生成Launcher来启动任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">NAME                                      READY   STATUS    RESTARTS   AGE<br>mpi-controller-manager-6fdb6ffd94-sjlrr   1/1     Running   0          18d<br>simple-train-worker-0                     1/1     Running   0          3m12s<br>simple-train-worker-1                     1/1     Running   0          3m12s<br>simple-train-worker-2                     1/1     Running   0          3m12s<br>simple-train-worker-3                     1/1     Running   0          3m11s<br>simple-train-worker-4                     0/1     Pending   0          3m11s<br></code></pre></td></tr></table></figure><p>可以查看Operator的日志进行验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">1.6604734402540798e+09INFOcontroller.mpijobworkers not ready&#123;<span class="hljs-string">&quot;reconciler group&quot;</span>: <span class="hljs-string">&quot;batch.test.bdap.com&quot;</span>, <span class="hljs-string">&quot;reconciler kind&quot;</span>: <span class="hljs-string">&quot;MPIJob&quot;</span>, <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;simple-train&quot;</span>, <span class="hljs-string">&quot;namespace&quot;</span>: <span class="hljs-string">&quot;sw-mpi-operator&quot;</span>&#125;<br></code></pre></td></tr></table></figure><p>并且Workers会一直占有GPU而等待，造成资源浪费。更坏的情况是，该任务在等待其他Pod完成后释放该资源，但不巧的是，占有该资源的Pod属于另外一个也处于等待状态的训练任务，那么就形成了死锁。</p><p>其形成原因是kubernetes的默认调度器以Pod为单位进行调度，如果我们以整个训练任务作为调度单位，要么给其中所有Pod都分配资源，要么就一个都不分配，“All or Nothing”，则可以解决这个问题，而gang调度正对应了该想法。</p><h1 id="如何自定义Scheduler"><a href="#如何自定义Scheduler" class="headerlink" title="如何自定义Scheduler"></a>如何自定义Scheduler</h1><p>我们先得知道如何去自定义kubernetes中的调度器。</p><details>    <summary>一些弯路</summary><!-- empty line -->    我在网上搜索到了这篇自定义调度器的[文章](http://dockone.io/article/2434717)，其提到自定义调度器的几种方式：<blockquote><ol><li>修改默认调度器的源代码，加入自己的调度算法，然后重新编译和部署调度器，论文<a href="https://link.springer.com/article/10.1007%2Fs11227-020-03427-3">kcss</a>和<a href="https://onlinelibrary.wiley.com/doi/10.1002/spe.2898">kubecg</a>中的调度器研究基于此方案实现；</li><li>开发自己的调度器，和默认调度器同时运行在集群中；</li><li>基于<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md">Kubernetes Scheduler Extender机制</a>，在扩展调度器中实现自定义算法，论文<a href="https://dl.acm.org/doi/10.1145/3407947.3407950">dynamic IO</a>中的算法实现基于这种方案。</li></ol></blockquote><p>其文章中使用的Extender方式，通过给默认调度器注册webhook进行，当默认调度器进行到Filter（预选）、Score（优选）、Bind阶段的时候会通过http请求调用webhook来获取结果。</p><p><img src="http://dockone.io/uploads/article/20211202/93df3081d0cd353cc56f2061607472c1.jpeg"></p><p>关于调度流程中的每个阶段介绍，可以参考<a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/">K8S Scheduling Framework</a></p><p>尝试与思考之后，我认为仅用Kubernetes Scheduler Extender方法来实现gang调度比较困难，因为它能控制的模块有限，并且我们不想手动去写分配资源的过程，而是想让调度器预分配资源后再根据情况做决定，因此需要在Reserve资源之后的Permit阶段进行修改。并且http请求的速度也比较慢。</p></details><!-- empty line --><p>后面发现一篇<a href="https://cloud.tencent.com/developer/article/1737844">文章</a>中提到四种方式自定义调度器：</p><blockquote><ul><li>default-scheduler recoding: 直接在Kubernetes默认scheduler基础上进行添加，然后重新编译kube-scheduler</li><li>standalone: 实现一个与kube-scheduler平行的custom scheduler，单独或者和默认kube-scheduler一起运行在集群中</li><li>scheduler extender: 实现一个”scheduler extender”，kube-scheduler会调用它(http&#x2F;https)作为默认调度算法(预选&amp;优选&amp;bind)的补充</li><li>scheduler framework: 实现scheduler framework plugins，重新编译kube-scheduler，类似于第一种方案，但是更加标准化，插件化</li></ul></blockquote><p>我认为最后scheduler framework的方式比较合适，我们只需要修改Permit阶段，检查每个Pod，根据其PodGroup对应的信息来判断就好。实现过程后面说，这里先说一下实现完后怎么使用。</p><ol><li>Pod如何找到调度器？Pod通过在spec中指定schedulerName来选择调度器</li><li>调度器如何注册到集群中？调度器本身是kube-scheduler的扩展版，而kube-scheduler启动时，会接收一个config文件路径作为参数，其内容为KubeSchedulerConfiguration类型的yaml文件，其指定了该调度器在集群中的schedulerName，而该名字需要与1中的名字一致。</li></ol><p>一开始调试的时候，可以跑一个与kube-scheduler平行的gang-scheduler，那么即便存在问题也不会影响原本的集群。虽然集群中同时存在多个scheduler的时候，可能会有同一块资源被分配给不同Pod的情况，不过问题不大，等到该调度器稳定后，将其替换默认的调度器就可以了。</p><details>    <summary>插件是write还是append？</summary><!-- empty line --><p>btw，我们写插件的时候，不需要担心自己的某个阶段（如Filter）会覆盖原有的阶段，除了QueueSort阶段外，我们的阶段都是附加的，这一点可以通过看源代码验证，可以看到对于Taint的检测也是通过插件的方式来进行的。</p><p><img src="/img/7.png"></p></details><!-- empty line --><h1 id="如何实现gang调度"><a href="#如何实现gang调度" class="headerlink" title="如何实现gang调度"></a>如何实现gang调度</h1><p>这部分我也没想到什么好思路，不过搜索到了阿里的一篇好文：<a href="https://www.infoq.cn/article/Q1l845yOl2GAF8WIVtCW">进击的 Kubernetes 调度系统（二）：支持批任务的 Coscheduling&#x2F;Gang scheduling</a>，发现他们已经早在两年前就已经完成并开源了这份工作，<a href="https://github.com/kubernetes-sigs/scheduler-plugins">源码</a>。墙裂推荐去看一下，本文的调度器也是仿照它造轮子，不过也存在一些改动。</p><p>博客中记录的是第一个版本的简单做法，使用label即可实现，<a href="https://github.com/kubernetes-sigs/scheduler-plugins/commit/98b12f1258ca407917cf71c298b91d7eb5b1706a">对应commit</a>，源码中目前是第二版本，使用CRD PodGroup实现，我猜想有两个原因：1. 防止同一个PodGroup中的Pod写的label声明的minAvailable不一致的情况。2. 解耦，podgroup本身不应该由scheduler管理，而应该由单独的operator进行管理。那为什么不用ConfigMap呢？我猜想是需要记录下PodGroup的一些Status。不过我们的使用情况比较简单，我偷点懒，就不写PodGroup的Operator了，简单使用ConfigMap来记录PodGroup信息，也能把PodGroup的控制权交给用户，让用户来维护，不需要scheduler来考虑创建与回收，也没有引入CRD管理，无需controller。</p><h2 id="Permit阶段"><a href="#Permit阶段" class="headerlink" title="Permit阶段"></a>Permit阶段</h2><p>我们的目标是把带有PodGroup标记的Pod都拦截下来，看看集群能否满足PodGroup整体的资源需求，如果满足条件就放行。因为到达这一步的Pod都已经经过Reserve阶段，资源已经预留下来，因此只需看到达该阶段的Pod是否达到数量要求就行，一共就三步：</p><ol><li>判断Pod的Label中是否标注了PodGroup，如果没有，说明是普通的Pod，不影响它，直接进入下一阶段</li><li>如果Label中写明了PodGroup，则记录该PodGroup对应需要的minAvailable</li><li>判断此时是否已经满足minAvailable，如果满足，则进入下一阶段，否则进入Wait状态</li></ol><p>其中“判断此时是否已经满足minAvailable”也包括三步：</p><ol><li>所有之前一定时间内的Wait状态的Pod都在一个队列中，我们查询该队列，给处于PodGroup的Pod计数。</li><li>查询当前集群，给属于该PodGroup的Pod计数，并且需要Pod处于Running状态下。</li><li>判断1和2的总和再加上当前Pod是否满足minAvailable</li></ol><details>    <summary>什么情况会出现该PodGroup已经有一些Pod在运行了，而一些Pod还在等待调度的队列中呢？</summary><!-- empty line -->    当第一次分配时就满足minAvailable条件，已经跑起来之后，后续又新增属于该PodGroup中的Pod，在调度这些新增Pod的时候。</details><!-- empty line --><h2 id="QueueSort阶段"><a href="#QueueSort阶段" class="headerlink" title="QueueSort阶段"></a>QueueSort阶段</h2><p>仅修改Permit还不够。原因：Default Scheduler的队列并不能感知PodGroup的信息，如果有多个PodGroup，那么不同PodGroup中的Pod有可能在队列里面交错，那么进入Reserve阶段的Pod也是交错的，因此还是产生死锁。因此我们需要添加QueueSort Plugin。</p><p>我们的目标是让不同的PodGroup中的Pod不能交错。做法：查询两个Pod对应的PodGroup标签，看是否属于某个PodGroup</p><ol><li>都不属于，则按照优先级，再按照create时间排序</li><li>都属于，则按照PodGroup的create时间排序，时间相同说明属于同一个PodGroup，则按照1进行</li><li>一个属于，一个不属于，则让不属于PodGroup的先行一步</li></ol><h1 id="如何使用gang-scheduler"><a href="#如何使用gang-scheduler" class="headerlink" title="如何使用gang-scheduler"></a>如何使用gang-scheduler</h1><p>安装与更加详细的例子，可以查看<a href="https://github.com/FFFFFaraway/gang-scheduler">gang-scheduler</a></p><p>本文就拿之前动机部分的GPU举例，新增一个ConfigMap记录PodGroup信息：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">gpu-pg</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">sw-mpi-operator</span><br><span class="hljs-attr">data:</span><br>  <span class="hljs-attr">minAvailable:</span> <span class="hljs-string">&quot;5&quot;</span><br>  <span class="hljs-attr">scheduleTimeoutSeconds:</span> <span class="hljs-string">&quot;20&quot;</span><br></code></pre></td></tr></table></figure><p>修改Pod信息，在label中添加<code>pod-group.scheduling.bdap.com/podgroup-configmap: gpu-pg</code>，以及修改调度器<code>schedulerName: gang-scheduler</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">batch.test.bdap.com/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">MPIJob</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">simple-train</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">sw-mpi-operator</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">numWorkers:</span> <span class="hljs-number">5</span><br>  <span class="hljs-attr">launcherTemplate:</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">containers:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">farawaya/horovod-torch-cpu</span><br>          <span class="hljs-attr">name:</span> <span class="hljs-string">horovod-master</span><br>  <span class="hljs-attr">workerTemplate:</span><br>    <span class="hljs-attr">metadata:</span><br>      <span class="hljs-attr">labels:</span><br>        <span class="hljs-attr">pod-group.scheduling.bdap.com/podgroup-configmap:</span> <span class="hljs-string">gpu-pg</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">schedulerName:</span> <span class="hljs-string">gang-scheduler</span><br>      <span class="hljs-attr">containers:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">args:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">sleep</span> <span class="hljs-string">infinity</span><br>          <span class="hljs-attr">command:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/sh</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">-c</span><br>          <span class="hljs-attr">image:</span> <span class="hljs-string">farawaya/horovod-torch-cuda113</span><br>          <span class="hljs-attr">name:</span> <span class="hljs-string">horovod-worker</span><br>          <span class="hljs-attr">resources:</span><br>            <span class="hljs-attr">limits:</span><br>              <span class="hljs-attr">nvidia.com/gpu:</span> <span class="hljs-number">1</span><br>      <span class="hljs-attr">tolerations:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span><br>          <span class="hljs-attr">key:</span> <span class="hljs-string">gpu</span><br>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span><br></code></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">NAME                                      READY   STATUS    RESTARTS   AGE<br>mpi-controller-manager-6fdb6ffd94-np6pg   1/1     Running   0          79s<br>simple-train-worker-0                     0/1     Pending   0          11s<br>simple-train-worker-1                     0/1     Pending   0          11s<br>simple-train-worker-2                     0/1     Pending   0          11s<br>simple-train-worker-3                     0/1     Pending   0          11s<br>simple-train-worker-4                     0/1     Pending   0          11s<br></code></pre></td></tr></table></figure><p>查看原因：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">Events:<br>  Type     Reason            Age   From            Message<br>  ----     ------            ----  ----            -------<br>  Warning  FailedScheduling  55s   gang-scheduler  rejected due to <span class="hljs-built_in">timeout</span> after waiting 20s at plugin sample<br>  Warning  FailedScheduling  33s   gang-scheduler  rejected due to <span class="hljs-built_in">timeout</span> after waiting 20s at plugin sample<br></code></pre></td></tr></table></figure><p>如果我们不创建PodGroup对应的ConfigMap就直接使用其作为label，也会出现提示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">Events:<br>  Type     Reason            Age   From            Message<br>  ----     ------            ----  ----            -------<br>  Warning  FailedScheduling  14s   gang-scheduler  running Permit plugin <span class="hljs-string">&quot;sample&quot;</span>: podgroup configmap not found, please create configmap first<br>  Warning  FailedScheduling  13s   gang-scheduler  running Permit plugin <span class="hljs-string">&quot;sample&quot;</span>: podgroup configmap not found, please create configmap first<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>使用kubebuilder编写自己的MPI Operator</title>
    <link href="/2022/07/27/write-mpi-operator/"/>
    <url>/2022/07/27/write-mpi-operator/</url>
    
    <content type="html"><![CDATA[<p>上次篇文章总结了MPI Operator需要做的事情，以及手动部署的实践。我们既然都可以手动部署出worker了，干脆再进一步，将该过程用代码实现，写一个MPI Operator。<a href="https://github.com/FFFFFaraway/MPI-Operator">Github</a></p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>目前的MPI Operator还存在哪些不足？</p><ol><li>代码较多。大概4000+行，本身属于kubeflow子项目，存在较多kubeflow依赖。不过该Operator的核心功能并不复杂。</li><li>缺少对于缺失Pod的恢复操作。代码中，看上去是ReplicaSet控制Worker&#x2F;Launcher Pod，实际上是Operator单独控制Pod，<a href="https://github.com/kubeflow/mpi-operator/blob/993b010e05c48c6f6ef5a5180362ccd3e716982e/pkg/controllers/v1/mpi_job_controller.go#L869">源码</a>中以Pod为单位进行Create，而非ReplicaSet。原因可能是ReplicaSet无法控制名字生成，从而无法事先写好ConfigMap中的hostfile。因此，只要有一个worker pod启动失败了，那么就需要手动删除MPIJob（触发Operator删除所有Pod），再重新Create MPIJob。</li><li>缺少根据MPIJob的修改更新Pod的控制操作。如果想修改MPIJob任务（比如修改workers执行的命令），同样也要手动删除现有MPIJob，再重新Create新的MPIJob。相比于ConfigMap，RBAC有Update，<a href="https://github.com/kubeflow/mpi-operator/blob/993b010e05c48c6f6ef5a5180362ccd3e716982e/pkg/controllers/v1/mpi_job_controller.go#L721">源码位置</a>，Worker没有，<a href="https://github.com/kubeflow/mpi-operator/blob/993b010e05c48c6f6ef5a5180362ccd3e716982e/pkg/controllers/v1/mpi_job_controller.go#L885">源码位置</a>。原因是Pod能修改的部分非常有限，一般做法是删除Pod再重建Pod，而这些操作通过Update Pod的控制器来触发（比如ReplicaSet，StatefulSet）。</li><li>一些不影响使用的小问题。比如上一篇文章中我们发现的一些不必要的配置，如kubectl-delivery中生成的hosts文件等。比如缺乏对于ConfigMap，ServiceAccount等资源的回收。比如等待所有workers就绪的功能可以交给Operator来实现，而非kubectl-delivery。</li></ol><p>我正好这几天学习了kubebuilder，尝试重构一下MPI Operator代码，写一个简化版的，学习其流程，以便之后做一些自定义的修改。</p><h2 id="需求与做法"><a href="#需求与做法" class="headerlink" title="需求与做法"></a>需求与做法</h2><p>Operator本质上是用代码来自动化部署与管理yaml，按照我们手动部署的思路来就行</p><ol><li><p>我想利用operator框架（如kubebuilder）来实现它，并且不使用kubeflow相关的依赖，而且尽量简单，目前该项目的go代码大概在800行之内，并且由于新建ConfigMap，RBAC（Role，RoleBinding，ServiceAccount）重复代码较多，实际逻辑部分的代码比较少也比较简单。</p></li><li><p>添加Worker Pod自动滚动更新（不包括Launcher Pod），也即当用户修改MPIJob中Workers对应的信息，比如修改了command，我们需要检测到这一点，并让workers pod可以滚动更新，同时还需保证workers pod的名字可以事先写入hostfile中，实现方式：</p><ol><li>通过一个控制器StatefulSet控制Workers Pod，而非直接控制Workers Pod，保证名字</li><li>当MPIJob更新时，会调用Reconcile函数，其中会Update workers对应的StatefulSet，从而让Pod滚动更新</li></ol></li><li><p>添加自动恢复（Worker statefulset与Launcher pod）。我们需要察觉StatefulSet或Pod缺失，并重新Create。以便如果想重新跑任务可以直接手动删除Launcher Pod，如果想重新拉取最新代码，可以手动删除Worker StatefulSet等。做法：</p><ol><li>给每个MPIJob添加了周期性检测，比如周期为1分钟调用一次Reconcile函数</li><li>在Reconcile函数中，发现缺失资源，则根据最新MPIJob内容新建资源</li></ol></li><li><p>添加等待Worker StatefulSet就绪才生成并运行Launcher Pod的功能。因为Worker Pod上通常需要安装一些运行环境，如requirements.txt，或者git clone代码，此时不能生成Launcher Pod执行任务。做法是在新建Launcher Pod之前检测Worker StatefulSet的Status，直到ReadyReplicas数量等于workers数量。</p></li><li><p>删除一些不必要的配置</p><ol><li><p>删除worker pod中的kubexec.sh挂载（我不确定。它似乎不影响horovodrun，却影响mpirun）</p></li><li><p>删除kubectl-delivery中两个功能：</p><ol><li>等待workers就绪。这个放到Operator中实现</li><li>在&#x2F;opt&#x2F;kube文件夹下生成hosts文件</li></ol><p>只保留其复制kubectl的功能（只需在Dockerfile中写即可）</p></li></ol></li></ol>]]></content>
    
    
    <categories>
      
      <category>uncategorized</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>MPI Operator原理分析</title>
    <link href="/2022/07/23/mpi-operator/"/>
    <url>/2022/07/23/mpi-operator/</url>
    
    <content type="html"><![CDATA[<p>MPI Operator是由kubeflow社区开发的，首先可以看看他们给的proposal文档<a href="https://github.com/kubeflow/community/blob/master/proposals/mpi-operator-proposal.md">https://github.com/kubeflow/community/blob/master/proposals/mpi-operator-proposal.md</a></p><p>之前我们使用过它来将模型训练任务部署到K8S中。这次我想来仔细看看它的实现原理。</p><h1 id="回顾上次使用"><a href="#回顾上次使用" class="headerlink" title="回顾上次使用"></a>回顾上次使用</h1><p>MPIJob是新建的CRD，用来描述MPI training的任务，比如以下是一个模型训练任务：</p><details>    <summary>MPIJob yaml</summary> <!-- Good place for a CTA (Call to Action) --> <!-- empty line *️⃣  -->  <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeflow.org/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">MPIJob</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">sw-simple-ml-gitlab</span><br><span class="hljs-attr">namespace:</span> <span class="hljs-string">mpi-operator</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">mpiReplicaSpecs:</span><br>    <span class="hljs-attr">Launcher:</span><br>      <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span><br>      <span class="hljs-attr">template:</span><br>        <span class="hljs-attr">spec:</span><br>          <span class="hljs-attr">containers:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">args:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">sleep</span> <span class="hljs-string">1m</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">mkdir</span> <span class="hljs-string">simple_ml</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">cd</span> <span class="hljs-string">simple_ml</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">horovodrun</span> <span class="hljs-string">-np</span> <span class="hljs-number">2</span> <span class="hljs-string">--hostfile</span><br>              <span class="hljs-string">/etc/mpi/hostfile</span> <span class="hljs-string">python</span> <span class="hljs-string">main_with_horovod.py</span><br>            <span class="hljs-attr">command:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;/bin/sh&quot;</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;-c&quot;</span><br>            <span class="hljs-attr">image:</span> <span class="hljs-string">coreharbor.bdap.com/library/horovod-sw-base</span><br>            <span class="hljs-attr">name:</span> <span class="hljs-string">horovod-master</span><br>    <span class="hljs-attr">Worker:</span><br>      <span class="hljs-attr">replicas:</span> <span class="hljs-number">2</span><br>      <span class="hljs-attr">template:</span><br>        <span class="hljs-attr">spec:</span><br>          <span class="hljs-attr">containers:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">args:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">git</span> <span class="hljs-string">-c</span> <span class="hljs-string">http.sslVerify=false</span> <span class="hljs-string">clone</span> <span class="hljs-string">&lt;https://gitlab.bdap.com/faraway/simple_ml.git&gt;</span><br>              <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">cd</span> <span class="hljs-string">simple_ml</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">pip</span> <span class="hljs-string">install</span> <span class="hljs-string">-r</span> <span class="hljs-string">requirements.txt</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">chmod</span> <span class="hljs-string">+x</span> <span class="hljs-string">init.sh</span><br>              <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">./init.sh</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">sleep</span> <span class="hljs-string">infinity</span><br>            <span class="hljs-attr">command:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;/bin/sh&quot;</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;-c&quot;</span><br>            <span class="hljs-attr">image:</span> <span class="hljs-string">coreharbor.bdap.com/library/horovod-sw-base</span><br>            <span class="hljs-attr">name:</span> <span class="hljs-string">horovod-worker</span><br>            <span class="hljs-attr">resources:</span><br>              <span class="hljs-attr">limits:</span><br>                <span class="hljs-attr">nvidia.com/gpu:</span> <span class="hljs-number">1</span><br>          <span class="hljs-attr">tolerations:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span><br>            <span class="hljs-attr">key:</span> <span class="hljs-string">gpu</span><br>            <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span><br>  <span class="hljs-attr">runPolicy:</span><br>    <span class="hljs-attr">cleanPodPolicy:</span> <span class="hljs-string">Running</span><br>  <span class="hljs-attr">slotsPerWorker:</span> <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure></details><!-- empty line *️⃣  --><p>根据该MPIJob的内容，我们期望其Controller可以生成一个Launcher Pod与多个Workers Pods来执行任务，比如：</p><details>    <summary>Launcher Pod（部分信息省略）</summary> <!-- Good place for a CTA (Call to Action) --> <!-- empty line *️⃣  -->  <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">sw-simple-ml-gitlab-launcher</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">args:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">sleep</span> <span class="hljs-string">1m</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">mkdir</span> <span class="hljs-string">simple_ml</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">cd</span> <span class="hljs-string">simple_ml</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">horovodrun</span> <span class="hljs-string">-np</span> <span class="hljs-number">2</span> <span class="hljs-string">--hostfile</span> <span class="hljs-string">/etc/mpi/hostfile</span><br>      <span class="hljs-string">python</span> <span class="hljs-string">main_with_horovod.py</span><br>    <span class="hljs-attr">command:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/sh</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">-c</span><br>    <span class="hljs-attr">env:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">OMPI_MCA_plm_rsh_agent</span><br>      <span class="hljs-attr">value:</span> <span class="hljs-string">/etc/mpi/kubexec.sh</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">OMPI_MCA_orte_default_hostfile</span><br>      <span class="hljs-attr">value:</span> <span class="hljs-string">/etc/mpi/hostfile</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">NVIDIA_VISIBLE_DEVICES</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">NVIDIA_DRIVER_CAPABILITIES</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">coreharbor.bdap.com/library/horovod-sw-base</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">horovod-master</span><br>    <span class="hljs-attr">volumeMounts:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/opt/kube</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">mpi-job-kubectl</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/mpi</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">mpi-job-config</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/run/secrets/kubernetes.io/serviceaccount</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">kube-api-access-jf6hw</span><br>      <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">initContainers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">env:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">TARGET_DIR</span><br>      <span class="hljs-attr">value:</span> <span class="hljs-string">/opt/kube</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">NAMESPACE</span><br>      <span class="hljs-attr">value:</span> <span class="hljs-string">mpi-operator</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">mpioperator/kubectl-delivery:latest</span><br>    <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">kubectl-delivery</span><br>    <span class="hljs-attr">resources:</span> <span class="hljs-string">...</span><br>    <span class="hljs-attr">terminationMessagePath:</span> <span class="hljs-string">/dev/termination-log</span><br>    <span class="hljs-attr">terminationMessagePolicy:</span> <span class="hljs-string">File</span><br>    <span class="hljs-attr">volumeMounts:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/opt/kube</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">mpi-job-kubectl</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/mpi</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">mpi-job-config</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/run/secrets/kubernetes.io/serviceaccount</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">kube-api-access-jf6hw</span><br>      <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">Never</span><br>  <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">sw-simple-ml-gitlab-launcher</span><br>  <span class="hljs-attr">volumes:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">emptyDir:</span> &#123;&#125;<br>    <span class="hljs-attr">name:</span> <span class="hljs-string">mpi-job-kubectl</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">configMap:</span><br>      <span class="hljs-attr">defaultMode:</span> <span class="hljs-number">420</span><br>      <span class="hljs-attr">items:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">kubexec.sh</span><br>        <span class="hljs-attr">mode:</span> <span class="hljs-number">365</span><br>        <span class="hljs-attr">path:</span> <span class="hljs-string">kubexec.sh</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">hostfile</span><br>        <span class="hljs-attr">mode:</span> <span class="hljs-number">292</span><br>        <span class="hljs-attr">path:</span> <span class="hljs-string">hostfile</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">discover_hosts.sh</span><br>        <span class="hljs-attr">mode:</span> <span class="hljs-number">365</span><br>        <span class="hljs-attr">path:</span> <span class="hljs-string">discover_hosts.sh</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">sw-simple-ml-gitlab-config</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">mpi-job-config</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">kube-api-access-jf6hw</span><br><span class="hljs-string">...</span><br></code></pre></td></tr></table></figure></details><!-- empty line *️⃣  --><details>    <summary>Worker Pod（部分信息省略）</summary><!-- empty line -->    <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">sw-simple-ml-gitlab-worker-0</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">args:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">git</span> <span class="hljs-string">-c</span> <span class="hljs-string">http.sslVerify=false</span> <span class="hljs-string">clone</span> <span class="hljs-string">&lt;https://gitlab.bdap.com/faraway/simple_ml.git&gt;</span><br>      <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">cd</span> <span class="hljs-string">simple_ml</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">pip</span> <span class="hljs-string">install</span> <span class="hljs-string">-r</span> <span class="hljs-string">requirements.txt</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">chmod</span> <span class="hljs-string">+x</span> <span class="hljs-string">init.sh</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">./init.sh</span><br>      <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">sleep</span> <span class="hljs-string">infinity</span><br>    <span class="hljs-attr">command:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/sh</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">-c</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">coreharbor.bdap.com/library/horovod-sw-base</span><br>    <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">Always</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">horovod-worker</span><br>    <span class="hljs-attr">resources:</span><br>      <span class="hljs-attr">limits:</span><br>        <span class="hljs-attr">nvidia.com/gpu:</span> <span class="hljs-string">&quot;1&quot;</span><br>      <span class="hljs-attr">requests:</span><br>        <span class="hljs-attr">nvidia.com/gpu:</span> <span class="hljs-string">&quot;1&quot;</span><br>    <span class="hljs-attr">volumeMounts:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/mpi</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">mpi-job-config</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/run/secrets/kubernetes.io/serviceaccount</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">kube-api-access-qw2jr</span><br>      <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">tolerations:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span><br>    <span class="hljs-attr">key:</span> <span class="hljs-string">gpu</span><br>    <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span><br>  <span class="hljs-attr">volumes:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">configMap:</span><br>      <span class="hljs-attr">defaultMode:</span> <span class="hljs-number">420</span><br>      <span class="hljs-attr">items:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">kubexec.sh</span><br>        <span class="hljs-attr">mode:</span> <span class="hljs-number">365</span><br>        <span class="hljs-attr">path:</span> <span class="hljs-string">kubexec.sh</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">sw-simple-ml-gitlab-config</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">mpi-job-config</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">kube-api-access-qw2jr</span><br>  <span class="hljs-string">...</span><br></code></pre></td></tr></table></figure></details><!-- empty line --><h1 id="从生成的Pod来猜测Controller做了什么"><a href="#从生成的Pod来猜测Controller做了什么" class="headerlink" title="从生成的Pod来猜测Controller做了什么"></a>从生成的Pod来猜测Controller做了什么</h1><p>首先Controller需要把MPIJob中的信息写入生成的Pod中。对于Worker Pod来说，就足够了，只需要等待Launcher发送命令。</p><p>而对于Launcher Pod来说，Controller还要为它做一些额外的事情，包括：</p><ol><li>添加volumes，&#x2F;etc&#x2F;mpi挂载一个ConfigMap，其中hostfile内容为workers的名字。第二个是&#x2F;opt&#x2F;kube，挂载emptyDir，用于接收initContainer的信息。</li><li>新增一个initContainer，名为kubectl-delivery，可以看到有一个环境变量<code>TARGET_DIR</code>为&#x2F;opt&#x2F;kube，在具体看其实现之前，我们可以猜测它的作用是将一些东西放到&#x2F;opt&#x2F;kube下，以供主container使用。</li></ol><h2 id="kubectl-delivery"><a href="#kubectl-delivery" class="headerlink" title="kubectl-delivery"></a>kubectl-delivery</h2><p>关键在于kubectl-delivery，现在我们通过代码，来具体看看kubectl-delivery的流程：</p><ol><li>从<a href="https://github.com/kubeflow/mpi-operator/blob/993b010e05c48c6f6ef5a5180362ccd3e716982e/cmd/kubectl-delivery/Dockerfile#L17">Dockerfile</a>中看它的启动命令为<code>cp /bin/kubectl /opt/kube/kubectl; /bin/kubectl-delivery -alsologtostderr</code>，将kubectl文件cp到&#x2F;opt&#x2F;kube下</li><li><a href="https://github.com/kubeflow/mpi-operator/blob/993b010e05c48c6f6ef5a5180362ccd3e716982e/cmd/kubectl-delivery/app/server.go#L95">源码</a>，解析&#x2F;etc&#x2F;mpi下的hostfile（由Controller生成，通过ConfigMap挂载），找到workers对应的Pod名字</li><li><a href="https://github.com/kubeflow/mpi-operator/blob/993b010e05c48c6f6ef5a5180362ccd3e716982e/pkg/controllers/kubectl_delivery/controller.go#L75">源码</a>，在<code>NewKubectlDeliveryController</code>中，将这些Pod加入watchedPods列表</li><li><a href="https://github.com/kubeflow/mpi-operator/blob/993b010e05c48c6f6ef5a5180362ccd3e716982e/pkg/controllers/kubectl_delivery/controller.go#L132">源码</a>，监听workers Pod，一旦正常运行则从watchedPods中删除</li><li><a href="https://github.com/kubeflow/mpi-operator/blob/993b010e05c48c6f6ef5a5180362ccd3e716982e/pkg/controllers/kubectl_delivery/controller.go#L150">源码</a>，等待workers全部正常运行后，在&#x2F;opt&#x2F;kube下生成hosts文件，里面记录了workers和自己（通过&#x2F;etc&#x2F;hosts最后一行获取）的Pod名字与对应IP</li></ol><p>因此，kubectl-delivery一共做了三件事：</p><ol><li>等待Workers Pod就绪</li><li>生成hosts文件，包含了Launcher和Workers Pod的名字与IP（不过我后续发现这个似乎没有发挥作用）</li><li>拷贝kubectl可执行文件</li></ol><p>kubectl与hosts文件都放在&#x2F;opt&#x2F;kube下，以供主container使用。</p><h2 id="Launcher-main-container"><a href="#Launcher-main-container" class="headerlink" title="Launcher main container"></a>Launcher main container</h2><p>但是问题来了。我们的Launcher主container中，是在哪里用了&#x2F;opt&#x2F;kube中的两个文件呢？</p><p>如果我们取消kubectl-delivery的执行，可以查看主container的报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh not successful <span class="hljs-keyword">for</span> host sw-simple-ml-gitlab-worker-1:<br>+ POD_NAME=sw-simple-ml-gitlab-worker-1<br>+ <span class="hljs-built_in">shift</span><br>+ /opt/kube/kubectl <span class="hljs-built_in">exec</span> sw-simple-ml-gitlab-worker-1 -- /bin/sh -c <span class="hljs-literal">true</span><br>/etc/mpi/kubexec.sh: 5: /opt/kube/kubectl: not found<br><br>ssh not successful <span class="hljs-keyword">for</span> host sw-simple-ml-gitlab-worker-0:<br>+ POD_NAME=sw-simple-ml-gitlab-worker-0<br>+ <span class="hljs-built_in">shift</span><br>+ /opt/kube/kubectl <span class="hljs-built_in">exec</span> sw-simple-ml-gitlab-worker-0 -- /bin/sh -c <span class="hljs-literal">true</span><br>/etc/mpi/kubexec.sh: 5: /opt/kube/kubectl: not found<br><br>Traceback (most recent call last):<br>  File <span class="hljs-string">&quot;/usr/local/bin/horovodrun&quot;</span>, line 8, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    sys.exit(run_commandline())<br>  File <span class="hljs-string">&quot;/usr/local/lib/python3.8/dist-packages/horovod/runner/launch.py&quot;</span>, line 773, <span class="hljs-keyword">in</span> run_commandline<br>    _run(args)<br>  File <span class="hljs-string">&quot;/usr/local/lib/python3.8/dist-packages/horovod/runner/launch.py&quot;</span>, line 763, <span class="hljs-keyword">in</span> _run<br>    <span class="hljs-built_in">return</span> _run_static(args)<br>  File <span class="hljs-string">&quot;/usr/local/lib/python3.8/dist-packages/horovod/runner/launch.py&quot;</span>, line 589, <span class="hljs-keyword">in</span> _run_static<br>    raise RuntimeError(<span class="hljs-string">&#x27;could not connect to some hosts via ssh&#x27;</span>)<br>RuntimeError: could not connect to some hosts via ssh<br></code></pre></td></tr></table></figure><p>可以看到是由<code>/etc/mpi/kubexec.sh</code>调用，那么谁在调用<code>kubexec.sh</code>呢？</p><p>查看Launcher Pod可以发现我们有一个环境变量<code>OMPI_MCA_plm_rsh_agent</code>，因此合理猜想，会不会是mpirun在使用之前会用该环境变量对应的脚本来发现worker？</p><p>查看mpirun中MCA参数的文档，<a href="https://docs.oracle.com/cd/E19708-01/821-1319-10/mca-params.html">https://docs.oracle.com/cd/E19708-01/821-1319-10/mca-params.html</a></p><p><img src="/img/1.png"></p><p>查找plm_rsh_agent参数的作用，<a href="https://www.open-mpi.org/faq/?category=rsh">https://www.open-mpi.org/faq/?category=rsh</a></p><p>我们发现这个参数是用来远程执行命令的连接方式前缀，取值一般为ssh或者rsh。</p><p>举例说明，比如我们想在服务器server-name执行ls somedir命令，则需要运行<code>$OMPI_MCA_plm_rsh_agent server-name ls somedir</code></p><p>我们具体看看<code>kubexec.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/sh</span><br><span class="hljs-built_in">set</span> -x<br>POD_NAME=<span class="hljs-variable">$1</span><br><span class="hljs-built_in">shift</span><br>/opt/kube/kubectl <span class="hljs-built_in">exec</span> <span class="hljs-variable">$&#123;POD_NAME&#125;</span> -- /bin/sh -c <span class="hljs-string">&quot;$*&quot;</span><br></code></pre></td></tr></table></figure><ol><li>set -x显示脚本的执行过程</li><li>POD_NAME为接受的第一个参数，也即连接的服务器名字</li><li>通过shift让参数后移（移除了第一个POD_NAME参数）。</li><li>让kubectl exec来远程执行后面的命令</li></ol><p>用kubectl exec的连接方式替换了ssh的连接方式（妙</p><p>还可以稍微在看一下运行的命令具体是什么，截取一部分（因为包含非常多不相干的环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash">/opt/kube/kubectl <span class="hljs-built_in">exec</span> sw-simple-ml-gitlab-worker-1 -- /bin/sh -c<br><span class="hljs-built_in">cd</span> /simple_ml &gt; /dev/null 2&gt;&amp;1 ;<br>HOROVOD_HOSTNAME=sw-simple-ml-gitlab-worker-1<br>HOROVOD_RANK=1<br>HOROVOD_SIZE=3<br>HOROVOD_LOCAL_RANK=0<br>HOROVOD_LOCAL_SIZE=1<br>HOROVOD_CROSS_RANK=1<br>HOROVOD_CROSS_SIZE=3<br>PWD=/simple_ml<br>HOROVOD_GLOO_RENDEZVOUS_ADDR=10.244.3.140<br>HOROVOD_GLOO_RENDEZVOUS_PORT=59558<br>HOROVOD_CONTROLLER=gloo<br>HOROVOD_CPU_OPERATIONS=gloo<br>HOROVOD_GLOO_IFACE=eth0<br>NCCL_SOCKET_IFNAME=eth0<br>python main_with_horovod.py<br></code></pre></td></tr></table></figure><p>其中HOROVOD_GLOO_RENDEZVOUS_ADDR&#x3D;10.244.3.140正好是Launcher Pod的IP，说明Workers Pod以Launcher Pod为中心进行通信。</p><p>现在还有一个问题是，&#x2F;opt&#x2F;kube文件夹下的hosts似乎没有用到。我尝试将其删除（做法是将kubectl-delivery的命令改为<code>cp /bin/kubectl /opt/kube/kubectl; /bin/kubectl-delivery -alsologtostderr; rm /opt/kube/hosts</code>），也能正确执行任务。因为kubectl exec只需要知道Pod Name就行，不需要IP。以及Worker中的kubexec.sh也不需要挂载（可以由后续我添加的worker验证）。</p><h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><p>我们已经大概理解了流程，这里举个例子来验证，同时也看看会不会有什么遗漏。比如，如果我想增加一个worker，不用MPI Operator，不修改MPIJob，我应该如何手动写yaml实现？</p><ol><li>创建一个Worker Pod，运行sleep infinity，我这里并没有挂载kubexec.sh以及hostfile</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">add-worker</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">mpi-operator</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">args:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">git</span> <span class="hljs-string">-c</span> <span class="hljs-string">http.sslVerify=false</span> <span class="hljs-string">clone</span> <span class="hljs-string">&lt;https://gitlab.bdap.com/faraway/simple_ml.git&gt;</span><br>      <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">cd</span> <span class="hljs-string">simple_ml</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">pip</span> <span class="hljs-string">install</span> <span class="hljs-string">-r</span> <span class="hljs-string">requirements.txt</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">chmod</span> <span class="hljs-string">+x</span> <span class="hljs-string">init.sh</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">./init.sh</span><br>      <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">sleep</span> <span class="hljs-string">infinity</span><br>    <span class="hljs-attr">command:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/sh</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">-c</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">coreharbor.bdap.com/library/horovod-sw-base</span><br>    <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">Always</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">horovod-worker</span><br>    <span class="hljs-attr">resources:</span><br>      <span class="hljs-attr">limits:</span><br>        <span class="hljs-attr">nvidia.com/gpu:</span> <span class="hljs-string">&quot;1&quot;</span><br>      <span class="hljs-attr">requests:</span><br>        <span class="hljs-attr">nvidia.com/gpu:</span> <span class="hljs-string">&quot;1&quot;</span><br>  <span class="hljs-attr">dnsConfig:</span><br>    <span class="hljs-attr">nameservers:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-number">10.105</span><span class="hljs-number">.222</span><span class="hljs-number">.6</span><br>  <span class="hljs-attr">tolerations:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span><br>    <span class="hljs-attr">key:</span> <span class="hljs-string">gpu</span><br>    <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span><br></code></pre></td></tr></table></figure><ol start="2"><li>将该Pod的名字加入ConfigMap的hostfile中。注意要先把ConfigMap的ownerReferences删除。防止Controller覆盖我们的修改。</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">data:</span><br>  <span class="hljs-attr">discover_hosts.sh:</span> <span class="hljs-bullet">-</span><br>    <span class="hljs-comment">#!/bin/sh</span><br>    <span class="hljs-string">echo</span> <span class="hljs-string">sw-simple-ml-gitlab-worker-0:1</span><br>    <span class="hljs-string">echo</span> <span class="hljs-string">sw-simple-ml-gitlab-worker-1:1</span><br>  <span class="hljs-attr">hostfile:</span> <br>    <span class="hljs-string">sw-simple-ml-gitlab-worker-0</span> <span class="hljs-string">slots=1</span><br>    <span class="hljs-string">sw-simple-ml-gitlab-worker-1</span> <span class="hljs-string">slots=1</span><br>    <span class="hljs-string">add-worker</span> <span class="hljs-string">slots=1</span><br>  <span class="hljs-attr">kubexec.sh:</span> <span class="hljs-bullet">-</span><br>    <span class="hljs-comment">#!/bin/sh</span><br>    <span class="hljs-string">set</span> <span class="hljs-string">-x</span><br>    <span class="hljs-string">POD_NAME=$1</span><br>    <span class="hljs-string">shift</span><br>    <span class="hljs-string">/opt/kube/kubectl</span> <span class="hljs-string">exec</span> <span class="hljs-string">$&#123;POD_NAME&#125;</span> <span class="hljs-string">--</span> <span class="hljs-string">/bin/sh</span> <span class="hljs-string">-c</span> <span class="hljs-string">&quot;$*&quot;</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">sw-simple-ml-gitlab-config</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">mpi-operator</span><br></code></pre></td></tr></table></figure><ol start="3"><li>新建一个Launcher Pod，并将运行的np改为3</li></ol><p>不过发现出错，应该是权限问题。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">Error from server (Forbidden): pods <span class="hljs-string">&quot;add-worker&quot;</span> is forbidden:<br>User <span class="hljs-string">&quot;system:serviceaccount:mpi-operator:sw-simple-ml-gitlab-launcher&quot;</span><br>cannot create resource <span class="hljs-string">&quot;pods/exec&quot;</span> <span class="hljs-keyword">in</span> API group <span class="hljs-string">&quot;&quot;</span> <span class="hljs-keyword">in</span> the namespace <span class="hljs-string">&quot;mpi-operator&quot;</span><br></code></pre></td></tr></table></figure><ol start="4"><li>需要修改RBAC中的Role，添加resourceNames，顺便去掉ownerReferences防止operator覆盖</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">sw-simple-ml-gitlab-launcher</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">mpi-operator</span><br><span class="hljs-attr">rules:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;&quot;</span><br>  <span class="hljs-attr">resources:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods</span><br>  <span class="hljs-attr">verbs:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;&quot;</span><br>  <span class="hljs-attr">resourceNames:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">sw-simple-ml-gitlab-worker-0</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">sw-simple-ml-gitlab-worker-1</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">add-worker</span><br>  <span class="hljs-attr">resources:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/exec</span><br>  <span class="hljs-attr">verbs:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">create</span><br></code></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>controller做的事情：</p><ol><li>根据MPIJob请求的workers的数量，生成其Pod名字，存储到ConfigMap中，事先写好kubexec.sh脚本</li><li>从MPIJob请求中提取workers Pod信息生成workers，应该没有其他额外操作了</li><li>从MPIJob请求中提取launcher Pod信息生成launcher<ol><li>给Launcher添加initContainer，它们两个共享&#x2F;opt&#x2F;kube</li><li>将ConfigMap挂载到&#x2F;etc&#x2F;mpi，添加环境变量以供mpirun调用&#x2F;etc&#x2F;mpi&#x2F;kubexec.sh</li><li>给Launcher Pod添加ServiceAccount，并给其添加exec workers pod的权限</li></ol></li></ol>]]></content>
    
    
    <categories>
      
      <category>uncategorized</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Elastic Cloud on Kubernetes (ECK)</title>
    <link href="/2022/07/08/elastic-cloud-on-kubernetes-eck/"/>
    <url>/2022/07/08/elastic-cloud-on-kubernetes-eck/</url>
    
    <content type="html"><![CDATA[<p>最终目的是：将K8S的Pod产生的日志收集到Elastic Search中，从而能通过搜索关键词快速定位问题。</p><h2 id="部署ES和Kibana"><a href="#部署ES和Kibana" class="headerlink" title="部署ES和Kibana"></a>部署ES和Kibana</h2><p>参考<a href="https://www.elastic.co/guide/en/cloud-on-k8s/2.1/k8s-overview.html"></a><a href="https://www.elastic.co/guide/en/cloud-on-k8s/2.1/k8s-overview.html">https://www.elastic.co/guide/en/cloud-on-k8s/2.1/k8s-overview.html</a></p><p>先部署CRD，和Operator，用于操作cluster。</p><p>之后通过自己定义cluster，创建EScluster，Kibana服务。</p><details>    <summary>Elasticsearch yaml</summary><!-- empty line -->    <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">elasticsearch.k8s.elastic.co/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Elasticsearch</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">quickstart-pvc</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">elastic-system</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">coreharbor.bdap.com/library/elasticsearch:8.2.0</span><br>  <span class="hljs-attr">nodeSets:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">config:</span><br>      <span class="hljs-attr">node.store.allow_mmap:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">count:</span> <span class="hljs-number">1</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">default</span><br>    <span class="hljs-attr">volumeClaimTemplates:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">metadata:</span><br>        <span class="hljs-attr">name:</span> <span class="hljs-string">elasticsearch-data</span><br>      <span class="hljs-attr">spec:</span><br>        <span class="hljs-attr">accessModes:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span><br>        <span class="hljs-attr">resources:</span><br>          <span class="hljs-attr">requests:</span><br>            <span class="hljs-attr">storage:</span> <span class="hljs-string">50Gi</span><br>  <span class="hljs-attr">version:</span> <span class="hljs-number">8.2</span><span class="hljs-number">.0</span><br></code></pre></td></tr></table></figure></details><!-- empty line --><details>    <summary>Kibana yaml</summary><!-- empty line -->    <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kibana.k8s.elastic.co/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Kibana</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">quickstart</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">elastic-system</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">config:</span><br>    <span class="hljs-attr">xpack.fleet.agentPolicies:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">id:</span> <span class="hljs-string">new-eck-fleet-server</span><br>      <span class="hljs-attr">is_default_fleet_server:</span> <span class="hljs-literal">true</span><br>      <span class="hljs-attr">monitoring_enabled:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">logs</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">metrics</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">fleet</span> <span class="hljs-string">server</span> <span class="hljs-string">policy</span><br>      <span class="hljs-attr">package_policies:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">id:</span> <span class="hljs-string">fleet_server-1</span><br>        <span class="hljs-attr">name:</span> <span class="hljs-string">fleet_server-1</span><br>        <span class="hljs-attr">package:</span><br>          <span class="hljs-attr">name:</span> <span class="hljs-string">fleet_server</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">id:</span> <span class="hljs-string">container-log-eck-agent</span><br>      <span class="hljs-attr">is_default:</span> <span class="hljs-literal">true</span><br>      <span class="hljs-attr">monitoring_enabled:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">logs</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">metrics</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">container</span> <span class="hljs-string">log</span> <span class="hljs-string">elastic</span> <span class="hljs-string">agent</span><br>      <span class="hljs-attr">package_policies:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">id:</span> <span class="hljs-string">elastic_agent-1</span><br>        <span class="hljs-attr">name:</span> <span class="hljs-string">elastic_agent-1</span><br>        <span class="hljs-attr">package:</span><br>          <span class="hljs-attr">name:</span> <span class="hljs-string">elastic_agent</span><br>      <span class="hljs-attr">unenroll_timeout:</span> <span class="hljs-number">900</span><br>    <span class="hljs-attr">xpack.fleet.agents.elasticsearch.hosts:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">&lt;https://quickstart-pvc-es-http.elastic-system.svc:9200&gt;</span><br>    <span class="hljs-attr">xpack.fleet.agents.fleet_server.hosts:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">&lt;https://fleet-server-quickstart-agent-http.elastic-system.svc:8220&gt;</span><br>    <span class="hljs-attr">xpack.fleet.packages:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">elastic_agent</span><br>      <span class="hljs-attr">version:</span> <span class="hljs-string">latest</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">fleet_server</span><br>      <span class="hljs-attr">version:</span> <span class="hljs-string">latest</span><br>  <span class="hljs-attr">count:</span> <span class="hljs-number">1</span><br>  <span class="hljs-attr">elasticsearchRef:</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">quickstart-pvc</span><br>  <span class="hljs-attr">http:</span><br>    <span class="hljs-attr">tls:</span><br>      <span class="hljs-attr">selfSignedCertificate:</span><br>        <span class="hljs-attr">disabled:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">coreharbor.bdap.com/elasticsearch/kibana:8.2.0</span><br>  <span class="hljs-attr">version:</span> <span class="hljs-number">8.2</span><span class="hljs-number">.0</span><br></code></pre></td></tr></table></figure></details><!-- empty line --><h2 id="部署Elastic-Agent"><a href="#部署Elastic-Agent" class="headerlink" title="部署Elastic Agent"></a>部署Elastic Agent</h2><p>Agent的yaml配置，【注意】mount的路径配置，对应一个k8s中的daemonst，收集每个机器上的log</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">agent.k8s.elastic.co/v1alpha1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Agent</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">elastic-agent-quickstart</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">elastic-system</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">daemonSet:</span><br>    <span class="hljs-attr">podTemplate:</span><br>      <span class="hljs-attr">spec:</span><br>        <span class="hljs-attr">automountServiceAccountToken:</span> <span class="hljs-literal">true</span><br>        <span class="hljs-attr">containers:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">agent</span><br>          <span class="hljs-attr">volumeMounts:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">&quot;/home/docker/lib/docker/containers&quot;</span><br>            <span class="hljs-attr">name:</span> <span class="hljs-string">varlibdockercontainers</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">&quot;/var/log/containers&quot;</span><br>            <span class="hljs-attr">name:</span> <span class="hljs-string">varlogcontainers</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">&quot;/var/log/pods&quot;</span><br>            <span class="hljs-attr">name:</span> <span class="hljs-string">varlogpods</span><br>        <span class="hljs-attr">securityContext:</span><br>          <span class="hljs-attr">runAsUser:</span> <span class="hljs-number">0</span><br>        <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">elastic-agent</span><br>        <span class="hljs-attr">volumes:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">hostPath:</span><br>            <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;/home/docker/lib/docker/containers&quot;</span><br>          <span class="hljs-attr">name:</span> <span class="hljs-string">varlibdockercontainers</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">hostPath:</span><br>            <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;/var/log/containers&quot;</span><br>          <span class="hljs-attr">name:</span> <span class="hljs-string">varlogcontainers</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">hostPath:</span><br>            <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;/var/log/pods&quot;</span><br>          <span class="hljs-attr">name:</span> <span class="hljs-string">varlogpods</span><br>  <span class="hljs-attr">fleetServerRef:</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">fleet-server-quickstart</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">elastic/elastic-agent:8.2.0</span><br>  <span class="hljs-attr">kibanaRef:</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">quickstart</span><br>  <span class="hljs-attr">mode:</span> <span class="hljs-string">fleet</span><br>  <span class="hljs-attr">version:</span> <span class="hljs-number">8.2</span><span class="hljs-number">.0</span><br></code></pre></td></tr></table></figure><p>这三个路径的关系：&#x2F;var&#x2F;log&#x2F;containers&#x2F;<em>.log为软连接，指向&#x2F;var&#x2F;log&#x2F;pods&#x2F;</em>&#x2F;<em>&#x2F;</em>.log软连接，再指向&#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers&#x2F;<em>&#x2F;</em>.log，为了让日志中包含pod名字，我们使用需要软连接，勾选Use Symlinks，且Kubernetes container log path为：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/var/</span>log<span class="hljs-regexp">/containers/</span>*<span class="hljs-variable">$&#123;kubernetes.container.id&#125;</span>.log<br></code></pre></td></tr></table></figure><p>【注意】由于裸机上的软连接最终指向【已经修改过路径的】&#x2F;home&#x2F;docker&#x2F;lib&#x2F;docker&#x2F;containers，因此，在container中也要mount到一样的路径下，而非默认的&#x2F;var&#x2F;lib。</p><details>    <summary>Fleet Server yaml</summary><!-- empty line -->    <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">agent.k8s.elastic.co/v1alpha1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Agent</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">fleet-server-quickstart</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">elastic-system</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">deployment:</span><br>    <span class="hljs-attr">podTemplate:</span><br>      <span class="hljs-attr">spec:</span><br>        <span class="hljs-attr">automountServiceAccountToken:</span> <span class="hljs-literal">true</span><br>        <span class="hljs-attr">securityContext:</span><br>          <span class="hljs-attr">runAsUser:</span> <span class="hljs-number">0</span><br>        <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">elastic-agent</span><br>    <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span><br>  <span class="hljs-attr">elasticsearchRefs:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">quickstart-pvc</span><br>  <span class="hljs-attr">fleetServerEnabled:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">elastic/elastic-agent:8.2.0</span><br>  <span class="hljs-attr">kibanaRef:</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">quickstart</span><br>  <span class="hljs-attr">mode:</span> <span class="hljs-string">fleet</span><br>  <span class="hljs-attr">version:</span> <span class="hljs-number">8.2</span><span class="hljs-number">.0</span><br></code></pre></td></tr></table></figure></details><!-- empty line --><p>tips:</p><p>之后对于elastic agent的各种配置，直接在kibana网页进行修改，且将配置存储到了数据库中。yaml中的后续修改可能不起作用。</p><p>比如：</p><ul><li>在integrations中kubernetes的proxy设置里，将localhost修改为：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-variable">$&#123;env.NODE_NAME&#125;</span>:10249<br></code></pre></td></tr></table></figure><ul><li>integrations中System的设置有默认server.example.com域名，可以选择关闭</li></ul><p>如果要删除现有policy以及agent，重新通过yaml进行部署的话，需要修改新部署的policy对应id，原因是修改过的配置被web服务存储到了数据库中，因此出现冲突。</p><p>最终效果：</p><ul><li>根据关键词，搜索日志</li></ul><p><img src="/img/4.png"></p><ul><li>滚动式日志</li></ul><p><img src="/img/5.png"></p>]]></content>
    
    
    <categories>
      
      <category>uncategorized</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Horovod on Kubernetes</title>
    <link href="/2022/07/07/horovod-on-kubernetes/"/>
    <url>/2022/07/07/horovod-on-kubernetes/</url>
    
    <content type="html"><![CDATA[<p>上一篇文章将Horovod封装到Docker image中，本文将进一步将其部署到Kubernetes集群上进行多机多GPU分布式训练。借助的工具为MPI-Operator，<a href="https://github.com/kubeflow/mpi-operator">https://github.com/kubeflow/mpi-operator</a></p><p>我们已经有horovod镜像了，部署到K8S上不是小菜一碟吗？然而没有那么简单，中间还有一些要注意的点。</p><h2 id="简单尝试"><a href="#简单尝试" class="headerlink" title="简单尝试"></a>简单尝试</h2><p>如果仅用kubectl来部署，做法可以是，把代码封装到镜像中，就像MPI-Operator中的example镜像那样做。接下来的过程可以参考Horovod in Docker，<a href="https://horovod.readthedocs.io/en/stable/docker_include.html#">https://horovod.readthedocs.io/en/stable/docker_include.html#</a></p><ul><li>部署一个master pod执行以下代码：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">horovodrun -np 16 -H host1:4,host2:4,host3:4,host4:4 -p 12345 python keras_mnist_advanced.py<br></code></pre></td></tr></table></figure><ul><li>部署多个worker pod执行：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash -c <span class="hljs-string">&quot;/usr/sbin/sshd -p 12345; sleep infinity&quot;</span><br></code></pre></td></tr></table></figure><p>不过需要注意的点是：</p><ol><li>我们需要事先将ssh key写到master与worker pod配置文件中，以便它们通信</li><li>master需要得知worker结点的名字来执行horovodrun命令，但是worker pod的名字一般是运行时生成的</li></ol><h2 id="借用MPI-Operator"><a href="#借用MPI-Operator" class="headerlink" title="借用MPI Operator"></a>借用MPI Operator</h2><p>这些操作都比较烦，因此MPI operator出现了。它具体做了哪些操作呢？可以参考，<a href="https://medium.com/kubeflow/introduction-to-kubeflow-mpi-operator-and-industry-adoption-296d5f2e6edc">https://medium.com/kubeflow/introduction-to-kubeflow-mpi-operator-and-industry-adoption-296d5f2e6edc</a>，以及<a href="https://github.com/kubeflow/community/blob/master/proposals/mpi-operator-proposal.md">https://github.com/kubeflow/community/blob/master/proposals/mpi-operator-proposal.md</a></p><p>大概过程：MPI使用kubectl exec完成第一次握手（从而可以让他们之间互相认识，后续进行通信），并且将workers pod对应的url给存储到configMap中，再挂载到master上。</p><p>此时，我们的训练流程为：</p><ul><li>修改代码</li><li>在horovod-base镜像上，重新build镜像（甚至可以安装自己的环境）</li><li>创建MPI Job</li></ul><p>问题在于，每次代码有变化都需要重新build以及push镜像，而由于horovod环境较为笨重导致镜像本身很大。因此这种方式还需要改进。</p><h2 id="借用Gitlab管理代码"><a href="#借用Gitlab管理代码" class="headerlink" title="借用Gitlab管理代码"></a>借用Gitlab管理代码</h2><p>有一种做法是，把代码单独拎出来，push到gitlab私服上，然后在MPI Job中通过git拉取。其中环境这一点通过requirements.txt的方式也保存下来。然后在每次创建MPI Job的时候后，在镜像中进行安装。</p><p>输入的数据，输出的模型参数等都存储在统一存储S3中。</p><p>此时，我们的训练流程为：</p><ul><li>本地修改代码，commit &amp; push到gitlab上</li><li>创建MPI Job，其中需要写git clone，pip install，horovodrun等命令</li></ul><p>MPI Job的例子：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeflow.org/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">MPIJob</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">mpi-operator</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">sw-simple-ml-gitlab</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">slotsPerWorker:</span> <span class="hljs-number">1</span><br>  <span class="hljs-attr">runPolicy:</span><br>    <span class="hljs-attr">cleanPodPolicy:</span> <span class="hljs-string">Running</span><br>  <span class="hljs-attr">mpiReplicaSpecs:</span><br>    <span class="hljs-attr">Launcher:</span><br>      <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span><br>      <span class="hljs-attr">template:</span><br>        <span class="hljs-attr">spec:</span><br>          <span class="hljs-attr">containers:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">coreharbor.bdap.com/library/horovod-sw-base</span><br>            <span class="hljs-attr">name:</span> <span class="hljs-string">horovod-master</span><br>            <span class="hljs-attr">command:</span> [<span class="hljs-string">&quot;/bin/sh&quot;</span>, <span class="hljs-string">&quot;-c&quot;</span>]<br>            <span class="hljs-attr">args:</span> [<span class="hljs-string">&quot;sleep 1m &amp;&amp; mkdir simple_ml &amp;&amp; cd simple_ml &amp;&amp; horovodrun -np 2 --hostfile /etc/mpi/hostfile python main_with_horovod.py&quot;</span>]<br>    <span class="hljs-attr">Worker:</span><br>      <span class="hljs-attr">replicas:</span> <span class="hljs-number">2</span><br>      <span class="hljs-attr">template:</span><br>        <span class="hljs-attr">spec:</span><br>          <span class="hljs-attr">dnsPolicy:</span> <span class="hljs-string">&quot;None&quot;</span><br>          <span class="hljs-attr">dnsConfig:</span><br>            <span class="hljs-attr">nameservers:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-number">10.105</span><span class="hljs-number">.222</span><span class="hljs-number">.6</span><br>          <span class="hljs-attr">containers:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">coreharbor.bdap.com/library/horovod-sw-base</span><br>            <span class="hljs-attr">name:</span> <span class="hljs-string">horovod-worker</span><br>            <span class="hljs-attr">command:</span> [<span class="hljs-string">&quot;/bin/sh&quot;</span>, <span class="hljs-string">&quot;-c&quot;</span>]<br>            <span class="hljs-attr">args:</span> [<span class="hljs-string">&quot;git -c http.sslVerify=false clone https://gitlab.bdap.com/faraway/simple_ml.git &amp;&amp; cd simple_ml &amp;&amp; pip install -r requirements.txt &amp;&amp; sleep infinity&quot;</span>]<br>            <span class="hljs-attr">resources:</span><br>              <span class="hljs-attr">limits:</span><br>                <span class="hljs-attr">nvidia.com/gpu:</span> <span class="hljs-number">1</span><br>          <span class="hljs-attr">tolerations:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span><br>            <span class="hljs-attr">key:</span> <span class="hljs-string">gpu</span><br>            <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span><br></code></pre></td></tr></table></figure><p>训练效果：</p><details>    <summary>训练过程的输出</summary><!-- empty line -->    <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs bash">[0]&lt;stderr&gt;:+ POD_NAME=sw-simple-ml-gitlab-worker-0<br>[0]&lt;stderr&gt;:+ <span class="hljs-built_in">shift</span><br>[0]&lt;stderr&gt;:+ /opt/kube/kubectl <span class="hljs-built_in">exec</span> sw-simple-ml-gitlab-worker-0 -- /bin/sh -c <span class="hljs-built_in">cd</span> /simple_ml &gt; /dev/null 2&gt;&amp;1 ; HOROVOD_HOSTNAME=sw-simple-ml-gitlab-worker-0 HOROVOD_RANK=0 HOROVOD_SIZE=2 HOROVOD_LOCAL_RANK=0 HOROVOD_LOCAL_SIZE=1 HOROVOD_CROSS_RANK=0 HOROVOD_CROSS_SIZE=2 LIBRARY_PATH=/usr/local/cuda/lib64/stubs KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 HOSTNAME=sw-simple-ml-gitlab-launcher LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 HOME=/root CUDA_VERSION=11.3.1 NVIDIA_REQUIRE_CUDA=<span class="hljs-string">&#x27;cuda&gt;=11.3 brand=tesla,driver&gt;=418,driver&lt;419 brand=tesla,driver&gt;=440,driver&lt;441 driver&gt;=450&#x27;</span> NVIDIA_DRIVER_CAPABILITIES=<span class="hljs-string">&#x27;&#x27;</span> KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1 PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin KUBERNETES_PORT_443_TCP_PORT=443 KUBERNETES_PORT_443_TCP_PROTO=tcp CUDNN_VERSION=8.2.0.53-1+cuda11.3 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_HOST=10.96.0.1 PWD=/simple_ml OMPI_MCA_orte_default_hostfile=/etc/mpi/hostfile OMPI_MCA_plm_rsh_agent=/etc/mpi/kubexec.sh NVIDIA_VISIBLE_DEVICES=<span class="hljs-string">&#x27;&#x27;</span> NCCL_VERSION=2.9.9-1+cuda11.3 TZ=Asia/Dubai LC_CTYPE=C.UTF-8 PYTHONUNBUFFERED=1 HOROVOD_GLOO_RENDEZVOUS_ADDR=10.244.3.228 HOROVOD_GLOO_RENDEZVOUS_PORT=56794 HOROVOD_CONTROLLER=gloo HOROVOD_CPU_OPERATIONS=gloo HOROVOD_GLOO_IFACE=eth0 NCCL_SOCKET_IFNAME=eth0 python main_with_horovod.py<br>[1]&lt;stderr&gt;:+ POD_NAME=sw-simple-ml-gitlab-worker-1<br>[1]&lt;stderr&gt;:+ <span class="hljs-built_in">shift</span><br>[1]&lt;stderr&gt;:+ /opt/kube/kubectl <span class="hljs-built_in">exec</span> sw-simple-ml-gitlab-worker-1 -- /bin/sh -c <span class="hljs-built_in">cd</span> /simple_ml &gt; /dev/null 2&gt;&amp;1 ; HOROVOD_HOSTNAME=sw-simple-ml-gitlab-worker-1 HOROVOD_RANK=1 HOROVOD_SIZE=2 HOROVOD_LOCAL_RANK=0 HOROVOD_LOCAL_SIZE=1 HOROVOD_CROSS_RANK=1 HOROVOD_CROSS_SIZE=2 LIBRARY_PATH=/usr/local/cuda/lib64/stubs KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 HOSTNAME=sw-simple-ml-gitlab-launcher LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 HOME=/root CUDA_VERSION=11.3.1 NVIDIA_REQUIRE_CUDA=<span class="hljs-string">&#x27;cuda&gt;=11.3 brand=tesla,driver&gt;=418,driver&lt;419 brand=tesla,driver&gt;=440,driver&lt;441 driver&gt;=450&#x27;</span> NVIDIA_DRIVER_CAPABILITIES=<span class="hljs-string">&#x27;&#x27;</span> KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1 PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin KUBERNETES_PORT_443_TCP_PORT=443 KUBERNETES_PORT_443_TCP_PROTO=tcp CUDNN_VERSION=8.2.0.53-1+cuda11.3 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_HOST=10.96.0.1 PWD=/simple_ml OMPI_MCA_orte_default_hostfile=/etc/mpi/hostfile OMPI_MCA_plm_rsh_agent=/etc/mpi/kubexec.sh NVIDIA_VISIBLE_DEVICES=<span class="hljs-string">&#x27;&#x27;</span> NCCL_VERSION=2.9.9-1+cuda11.3 TZ=Asia/Dubai LC_CTYPE=C.UTF-8 PYTHONUNBUFFERED=1 HOROVOD_GLOO_RENDEZVOUS_ADDR=10.244.3.228 HOROVOD_GLOO_RENDEZVOUS_PORT=56794 HOROVOD_CONTROLLER=gloo HOROVOD_CPU_OPERATIONS=gloo HOROVOD_GLOO_IFACE=eth0 NCCL_SOCKET_IFNAME=eth0 python main_with_horovod.py<br>[0]&lt;stdout&gt;:Training. Epoch 0, MSE loss: 1338.4868140452961, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 0, MSE loss: 1148.9670435080386, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 1, MSE loss: 935.5324933822116, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 1, MSE loss: 934.2259948853756, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 2, MSE loss: 654.0407885544738, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 2, MSE loss: 633.2420742589119, Worker: 1<br>[1]&lt;stdout&gt;:Training. Epoch 3, MSE loss: 599.6154769317578, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 3, MSE loss: 593.8755020723866, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 4, MSE loss: 574.4224909156511, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 4, MSE loss: 483.1180045366194, Worker: 0<br>[0]&lt;stdout&gt;:Training. Epoch 5, MSE loss: 501.9576651239583, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 5, MSE loss: 561.3756192270189, Worker: 1<br>[1]&lt;stdout&gt;:Training. Epoch 6, MSE loss: 543.3503851517656, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 6, MSE loss: 461.72367964229545, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 7, MSE loss: 599.5701360625471, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 7, MSE loss: 477.2382655836473, Worker: 0<br>[0]&lt;stdout&gt;:Training. Epoch 8, MSE loss: 467.4026207899954, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 8, MSE loss: 489.72919646231924, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 9, MSE loss: 496.021579158862, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 9, MSE loss: 466.33928261113863, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 10, MSE loss: 473.9692597730085, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 10, MSE loss: 484.8857977675169, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 11, MSE loss: 472.89639633833195, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 11, MSE loss: 468.5362197326799, Worker: 1<br>[1]&lt;stdout&gt;:Training. Epoch 12, MSE loss: 460.76999270017814, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 12, MSE loss: 416.7220215983699, Worker: 0<br>[0]&lt;stdout&gt;:Training. Epoch 13, MSE loss: 451.9702810886584, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 13, MSE loss: 439.8771825716403, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 14, MSE loss: 366.88131853180283, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 14, MSE loss: 470.20061697393044, Worker: 1<br>[1]&lt;stdout&gt;:Training. Epoch 15, MSE loss: 431.8841860381735, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 15, MSE loss: 395.79066620089884, Worker: 0<br>[0]&lt;stdout&gt;:Training. Epoch 16, MSE loss: 390.8923997512978, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 16, MSE loss: 425.90262653747106, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 17, MSE loss: 516.2323905151153, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 17, MSE loss: 355.790610972445, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 18, MSE loss: 501.96195440185153, Worker: 0<br>[1]&lt;stdout&gt;:Training. Epoch 18, MSE loss: 442.8678330586471, Worker: 1<br>[1]&lt;stdout&gt;:Training. Epoch 19, MSE loss: 445.24695194348794, Worker: 1<br>[0]&lt;stdout&gt;:Training. Epoch 19, MSE loss: 441.0054695299096, Worker: 0<br>[0]&lt;stdout&gt;:Testing. MSE loss: 219.68063354492188, Worker: 0<br>[1]&lt;stdout&gt;:Testing. MSE loss: 219.68063354492188, Worker: 1<br></code></pre></td></tr></table></figure></details><!-- empty line --><p>目前有个难受的点是，master需要等待worker安装完环境才能运行horovodrun命令，因此写了一个<strong>sleep 1m</strong>。后续应该可以用一些Pod之间的通信方式解决这个。</p><p>更新，可以通过readinessProbe解决。<a href="https://github.com/FFFFFaraway/MPI-Operator/blob/d180e938502e5af56ecc3105d4f904d9e14cb712/config/samples/training_job_cpu.yaml#L38">例子</a></p>]]></content>
    
    
    <categories>
      
      <category>uncategorized</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Horovod in Docker</title>
    <link href="/2022/06/19/horovod-in-docker/"/>
    <url>/2022/06/19/horovod-in-docker/</url>
    
    <content type="html"><![CDATA[<p>Horovod官方镜像安装，参考<a href="https://horovod.readthedocs.io/en/stable/docker_include.html">https://horovod.readthedocs.io/en/stable/docker_include.html</a>。</p><p>但由于官方镜像没有显示CUDA等版本，因此很可能会出现版本不兼容的情况，需要根据自己的GPU来build镜像。参考，<a href="https://github.com/determined-ai/horovod/blob/master/docs/docker.rst">https://github.com/determined-ai/horovod/blob/master/docs/docker.rst</a></p><h2 id="检查版本兼容"><a href="#检查版本兼容" class="headerlink" title="检查版本兼容"></a>检查版本兼容</h2><p>写Dockerfile中，需要注意各种库的版本。</p><p>base的版本我选择11.3的cuda，ubuntu平台</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dockerfile"><span class="hljs-keyword">FROM</span> nvidia/cuda:<span class="hljs-number">11.3</span>.<span class="hljs-number">1</span>-devel-ubuntu20.<span class="hljs-number">04</span><br></code></pre></td></tr></table></figure><p><strong>cuda的版本</strong></p><p>汇总参考，<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html</a></p><p>例如以下链接，对应的版本分别为</p><p><a href="https://developer.nvidia.com/compute/cudnn/secure/8.4.0/local/_installers/11.6/cudnn-local-repo-ubuntu2004-8.4.0.27/_1.0-1/_amd64.deb">https://developer.nvidia.com/compute/cudnn/secure/8.4.0/local\_installers/11.6/cudnn-local-repo-ubuntu2004-8.4.0.27\_1.0-1\_amd64.deb</a></p><p><a href="https://developer.nvidia.com/compute/machine-learning/cudnn/secure/8.2.0.53/11.3/_04222021/Ubuntu20/_04-x64/libcudnn8/_8.2.0.53-1+cuda11.3/_amd64.deb">https://developer.nvidia.com/compute/machine-learning/cudnn/secure/8.2.0.53/11.3\_04222021/Ubuntu20\_04-x64/libcudnn8\_8.2.0.53-1+cuda11.3\_amd64.deb</a></p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs dockerfile"><span class="hljs-number">8.4</span>.<span class="hljs-number">0.27</span>-<span class="hljs-number">1</span>+cuda11.<span class="hljs-number">6</span><br><span class="hljs-number">8.2</span>.<span class="hljs-number">0.53</span>-<span class="hljs-number">1</span>+cuda11.<span class="hljs-number">3</span><br></code></pre></td></tr></table></figure><p><strong>cudnn的版本</strong></p><p>汇总，参考，<a href="https://developer.nvidia.com/rdp/cudnn-archive">https://developer.nvidia.com/rdp/cudnn-archive</a></p><p><strong>NCCL版本</strong></p><p>汇总，参考<a href="https://docs.nvidia.com/deeplearning/nccl/release-notes/index.html">https://docs.nvidia.com/deeplearning/nccl/release-notes/index.html</a></p><p>注意查看兼容性：</p><p><img src="/img/2.png"></p><p>最终选择cuda11.3，nccl2.9.9</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs dockerfile"><span class="hljs-keyword">ENV</span> CUDNN_VERSION=<span class="hljs-number">8.2</span>.<span class="hljs-number">0.53</span>-<span class="hljs-number">1</span>+cuda11.<span class="hljs-number">3</span><br><span class="hljs-keyword">ENV</span> NCCL_VERSION=<span class="hljs-number">2.9</span>.<span class="hljs-number">9</span>-<span class="hljs-number">1</span>+cuda11.<span class="hljs-number">3</span><br><span class="hljs-comment"># Set default shell to /bin/bash</span><br><span class="hljs-keyword">SHELL</span><span class="language-bash"> [<span class="hljs-string">&quot;/bin/bash&quot;</span>, <span class="hljs-string">&quot;-cu&quot;</span>]</span><br><span class="hljs-keyword">RUN</span><span class="language-bash"> apt-get update --allow-insecure-repositories &amp;&amp; apt-get install -y --allow-unauthenticated --allow-downgrades --allow-change-held-packages --no-install-recommends \</span><br><span class="language-bash">        build-essential \</span><br><span class="language-bash">        cmake \</span><br><span class="language-bash">        g++-9 \</span><br><span class="language-bash">        git \</span><br><span class="language-bash">        curl \</span><br><span class="language-bash">        vim \</span><br><span class="language-bash">        wget \</span><br><span class="language-bash">        ca-certificates \</span><br><span class="language-bash">        libcudnn8=<span class="hljs-variable">$&#123;CUDNN_VERSION&#125;</span> \</span><br><span class="language-bash">        libnccl2=<span class="hljs-variable">$&#123;NCCL_VERSION&#125;</span> \</span><br><span class="language-bash">        libnccl-dev=<span class="hljs-variable">$&#123;NCCL_VERSION&#125;</span> \</span><br><span class="language-bash">        libjpeg-dev \</span><br><span class="language-bash">        libpng-dev \</span><br><span class="language-bash">        python-is-python3 \</span><br><span class="language-bash">        python3-pip \</span><br><span class="language-bash">        python3-dev \</span><br><span class="language-bash">        python3-distutils \</span><br><span class="language-bash">        librdmacm1 \</span><br><span class="language-bash">        libibverbs1 \</span><br><span class="language-bash">        ibverbs-providers</span><br></code></pre></td></tr></table></figure><p>build过程中，可能会在tzdata的安装上hang，（猜测原因是无法接收输入）添加时区：</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs dockerfile"><span class="hljs-keyword">ENV</span> TZ=Asia/Dubai<br><span class="hljs-keyword">RUN</span><span class="language-bash"> <span class="hljs-built_in">ln</span> -snf /usr/share/zoneinfo/<span class="hljs-variable">$TZ</span> /etc/localtime &amp;&amp; <span class="hljs-built_in">echo</span> <span class="hljs-variable">$TZ</span> &gt; /etc/timezone</span><br></code></pre></td></tr></table></figure><p>pip安装时，使用国内镜像，不然很慢，可以直接写到Dockerfile中：</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dockerfile"><span class="hljs-keyword">RUN</span><span class="language-bash"> pip config <span class="hljs-built_in">set</span> global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br></code></pre></td></tr></table></figure><p>安装Pytorch</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs dockerfile"><span class="hljs-keyword">RUN</span><span class="language-bash"> PYTAGS=$(python -c <span class="hljs-string">&quot;from packaging import tags; tag = list(tags.sys_tags())[0]; print(f&#x27;&#123;tag.interpreter&#125;-&#123;tag.abi&#125;&#x27;)&quot;</span>) &amp;&amp; \</span><br><span class="language-bash">pip install https://download.pytorch.org/whl/cu113/torch-<span class="hljs-variable">$&#123;PYTORCH_VERSION&#125;</span>%2Bcu113-<span class="hljs-variable">$&#123;PYTAGS&#125;</span>-linux_x86_64.whl</span><br></code></pre></td></tr></table></figure><p>安装horovod</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs dockerfile"><span class="hljs-keyword">RUN</span><span class="language-bash"> HOROVOD_GPU_OPERATIONS=NCCL \</span><br><span class="language-bash">    HOROVOD_WITH_PYTORCH=1 \</span><br><span class="language-bash">    HOROVOD_WITHOUT_MPI=1 \</span><br><span class="language-bash">         pip install --no-cache-dir horovod</span><br><span class="hljs-comment"># Install OpenSSH for MPI to communicate between containers</span><br><span class="hljs-keyword">RUN</span><span class="language-bash"> apt-get install -y --no-install-recommends openssh-client openssh-server &amp;&amp; \</span><br><span class="language-bash">    <span class="hljs-built_in">mkdir</span> -p /var/run/sshd</span><br><span class="hljs-comment"># Allow OpenSSH to talk to containers without asking for confirmation</span><br><span class="hljs-keyword">RUN</span><span class="language-bash"> <span class="hljs-built_in">cat</span> /etc/ssh/ssh_config  grep -v StrictHostKeyChecking &gt; /etc/ssh/ssh_config.new &amp;&amp; \</span><br><span class="language-bash">    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;    StrictHostKeyChecking no&quot;</span> &gt;&gt; /etc/ssh/ssh_config.new &amp;&amp; \</span><br><span class="language-bash">    <span class="hljs-built_in">mv</span> /etc/ssh/ssh_config.new /etc/ssh/ssh_config</span><br></code></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>生成数据，train.csv，test.csv，保存到S3存储中</p><details>    <summary>生成数据的python代码</summary><!-- empty line --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> s3_utils <span class="hljs-keyword">import</span> s3_save_pickle<br><span class="hljs-keyword">import</span> boto3<br><br>N = <span class="hljs-number">1000</span><br>ratio = <span class="hljs-number">0.8</span><br><br>a = np.random.uniform(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, N)<br>b = np.random.uniform(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, N)<br>noise = np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, N)<br>c = a + b + a * b + noise<br>data = np.stack([a, b, c], axis=-<span class="hljs-number">1</span>)<br><br>pos = <span class="hljs-built_in">int</span>(N * ratio)<br>train_set = data[:pos]<br>test_set = data[pos:]<br><br><span class="hljs-comment"># local save</span><br><span class="hljs-comment"># np.savetxt(&#x27;train.csv&#x27;, train_set, delimiter=&#x27;,&#x27;)</span><br><span class="hljs-comment"># np.savetxt(&#x27;test.csv&#x27;, test_set, delimiter=&#x27;,&#x27;)</span><br><br>session = boto3.session.Session()<br><br>s3_client = session.client(<br>    service_name=<span class="hljs-string">&#x27;s3&#x27;</span>,<br>    aws_access_key_id=<span class="hljs-string">&#x27;XXXX&#x27;</span>,<br>    aws_secret_access_key=<span class="hljs-string">&#x27;XXXX&#x27;</span>,<br>    endpoint_url=<span class="hljs-string">&#x27;http://10.105.222.7:24850&#x27;</span>,<br>)<br><br>s3_save_pickle(s3_client, train_set, <span class="hljs-string">&#x27;songwei&#x27;</span>, <span class="hljs-string">&#x27;simple_ml/train.pkl&#x27;</span>)<br>s3_save_pickle(s3_client, test_set, <span class="hljs-string">&#x27;songwei&#x27;</span>, <span class="hljs-string">&#x27;simple_ml/test.pkl&#x27;</span>)<br></code></pre></td></tr></table></figure></details><!-- empty line --><h1 id="单个CPU或者GPU测试"><a href="#单个CPU或者GPU测试" class="headerlink" title="单个CPU或者GPU测试"></a>单个CPU或者GPU测试</h1><details>    <summary>测试代码</summary><!-- empty line --><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">from</span> s3_utils <span class="hljs-keyword">import</span> s3_load_pickle, s3_save_file, s3_save_model<br><span class="hljs-keyword">import</span> boto3<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> MSELoss<br><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> Adam<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> TensorDataset, DataLoader<br><br>device = torch.device(<span class="hljs-string">&#x27;cuda:0&#x27;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>)<br><span class="hljs-comment"># define model</span><br>in_dim, hidden_dim, out_dim = <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span><br>hp = json.load(<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;hp.json&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>))<br><br>model = nn.Sequential(<br>    nn.Linear(in_dim, hidden_dim),<br>    nn.GELU(),<br>    nn.Dropout(hp[<span class="hljs-string">&quot;dropout&quot;</span>]),<br>    nn.Linear(hidden_dim, hidden_dim),<br>    nn.GELU(),<br>    nn.Dropout(hp[<span class="hljs-string">&quot;dropout&quot;</span>]),<br>    nn.Linear(hidden_dim, out_dim)<br>)<br>model.to(device)<br><br><span class="hljs-comment"># prepare data</span><br>session = boto3.session.Session()<br><br>s3_client = session.client(<br>    service_name=<span class="hljs-string">&#x27;s3&#x27;</span>,<br>    aws_access_key_id=<span class="hljs-string">&#x27;XXXX&#x27;</span>,<br>    aws_secret_access_key=<span class="hljs-string">&#x27;XXXX&#x27;</span>,<br>    endpoint_url=<span class="hljs-string">&#x27;http://10.105.222.7:24850&#x27;</span>,<br>)<br><br>train_tensor = torch.tensor(s3_load_pickle(s3_client, <span class="hljs-string">&#x27;songwei&#x27;</span>, <span class="hljs-string">&#x27;simple_ml/train.pkl&#x27;</span>), dtype=torch.<span class="hljs-built_in">float</span>)<br>test_tensor = torch.tensor(s3_load_pickle(s3_client, <span class="hljs-string">&#x27;songwei&#x27;</span>, <span class="hljs-string">&#x27;simple_ml/test.pkl&#x27;</span>), dtype=torch.<span class="hljs-built_in">float</span>)<br><br>train_dataset = TensorDataset(train_tensor[:, :-<span class="hljs-number">1</span>], train_tensor[:, -<span class="hljs-number">1</span>:])<br>test_dataset = TensorDataset(test_tensor[:, :-<span class="hljs-number">1</span>], test_tensor[:, -<span class="hljs-number">1</span>:])<br><br>train_dl = DataLoader(train_dataset)<br>test_dl = DataLoader(test_dataset)<br><br>opt = Adam(model.parameters(), lr=hp[<span class="hljs-string">&quot;lr&quot;</span>])<br>loss_fn = MSELoss()<br>epoch = <span class="hljs-number">20</span><br><br><span class="hljs-comment"># train and test</span><br>model.train()<br><span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>    avg_loss = []<br>    <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> train_dl:<br>        x = x.to(device)<br>        y = y.to(device)<br>        opt.zero_grad()<br>        p = model(x)<br>        loss = loss_fn(p, y)<br>        loss.backward()<br>        opt.step()<br>        avg_loss.append(loss.item())<br>    avg_loss = <span class="hljs-built_in">sum</span>(avg_loss) / <span class="hljs-built_in">len</span>(avg_loss)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Training. Epoch <span class="hljs-subst">&#123;e&#125;</span>, MSE loss: <span class="hljs-subst">&#123;avg_loss&#125;</span>&#x27;</span>)<br><br>model.<span class="hljs-built_in">eval</span>()<br><span class="hljs-keyword">with</span> torch.no_grad():<br>    avg_loss = []<br>    <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> test_dl:<br>        x = x.to(device)<br>        y = y.to(device)<br>        p = model(x)<br>        loss = loss_fn(p, y)<br>        avg_loss.append(loss.item())<br>    avg_loss = <span class="hljs-built_in">sum</span>(avg_loss) / <span class="hljs-built_in">len</span>(avg_loss)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Testing. MSE loss: <span class="hljs-subst">&#123;avg_loss&#125;</span>&#x27;</span>)<br><br>s3_save_model(s3_client, model, <span class="hljs-string">&#x27;songwei&#x27;</span>, <span class="hljs-string">&#x27;simple_ml/model_save/single/model.pt&#x27;</span>)<br>s3_save_file(s3_client, <span class="hljs-string">&#x27;config.pbtxt&#x27;</span>, <span class="hljs-string">&#x27;songwei&#x27;</span>, <span class="hljs-string">&#x27;simple_ml/model_save/single/config.pbtxt&#x27;</span>)<br></code></pre></td></tr></table></figure></details><!-- empty line --><h1 id="测试NCCL"><a href="#测试NCCL" class="headerlink" title="测试NCCL"></a>测试NCCL</h1><p>开启Debug模式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> NCCL_DEBUG=INFO<br></code></pre></td></tr></table></figure><p>查看目前ld能找到的lib，需要让ldconfig能找到nccl</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ldconfig -p<br></code></pre></td></tr></table></figure><p>测试参考，nccl-test，<a href="https://github.com/NVIDIA/nccl-tests.git">https://github.com/NVIDIA/nccl-tests.git</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> &lt;https://github.com/NVIDIA/nccl-tests.git&gt;<br><span class="hljs-built_in">cd</span> nccl-tests<br>make<br>./build/all_reduce_perf -b 8 -e 256M -f 2 -g 4<br></code></pre></td></tr></table></figure><p>【注意】nccl-test在docker中运行时，需要添加以下参数shm-size，不然可能有bus error</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker run -it --shm-size 8G --<span class="hljs-built_in">rm</span> myhorovod bash<br></code></pre></td></tr></table></figure><h1 id="单机多GPU带horovod的python代码"><a href="#单机多GPU带horovod的python代码" class="headerlink" title="单机多GPU带horovod的python代码"></a>单机多GPU带horovod的python代码</h1><p>保证horovod在python中的写法没问题，以及horovod的安装没有问题</p><details>    <summary>测试代码</summary><!-- empty line -->    <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> MSELoss<br><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> Adam<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> TensorDataset, DataLoader<br><span class="hljs-keyword">from</span> torch.utils.data.distributed <span class="hljs-keyword">import</span> DistributedSampler<br><span class="hljs-keyword">import</span> horovod.torch <span class="hljs-keyword">as</span> hvd<br><span class="hljs-keyword">from</span> s3_utils <span class="hljs-keyword">import</span> s3_load_pickle, s3_save_model, s3_save_file<br><span class="hljs-keyword">import</span> boto3<br><br><span class="hljs-comment"># prepare data</span><br>session = boto3.session.Session()<br><br>s3_client = session.client(<br>    service_name=<span class="hljs-string">&#x27;s3&#x27;</span>,<br>    aws_access_key_id=<span class="hljs-string">&#x27;XXXX&#x27;</span>,<br>    aws_secret_access_key=<span class="hljs-string">&#x27;XXXX&#x27;</span>,<br>    endpoint_url=<span class="hljs-string">&#x27;http://10.105.222.7:24850&#x27;</span>,<br>)<br><br>train_tensor = torch.tensor(s3_load_pickle(s3_client, <span class="hljs-string">&#x27;songwei&#x27;</span>, <span class="hljs-string">&#x27;simple_ml/train.pkl&#x27;</span>), dtype=torch.<span class="hljs-built_in">float</span>)<br>test_tensor = torch.tensor(s3_load_pickle(s3_client, <span class="hljs-string">&#x27;songwei&#x27;</span>, <span class="hljs-string">&#x27;simple_ml/test.pkl&#x27;</span>), dtype=torch.<span class="hljs-built_in">float</span>)<br><br>train_dataset = TensorDataset(train_tensor[:, :-<span class="hljs-number">1</span>], train_tensor[:, -<span class="hljs-number">1</span>:])<br>test_dataset = TensorDataset(test_tensor[:, :-<span class="hljs-number">1</span>], test_tensor[:, -<span class="hljs-number">1</span>:])<br><br><span class="hljs-comment">######################################################</span><br>hvd.init()<br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    torch.cuda.set_device(hvd.local_rank())<br>torch.set_num_threads(<span class="hljs-number">1</span>)<br><br>train_sampler = DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())<br>train_dl = DataLoader(train_dataset, sampler=train_sampler)<br><br>test_sampler = DistributedSampler(test_dataset, num_replicas=hvd.size(), rank=hvd.rank())<br>test_dl = DataLoader(test_dataset, sampler=test_sampler)<br><span class="hljs-comment">######################################################</span><br><br><span class="hljs-comment"># define model</span><br>in_dim, hidden_dim, out_dim = <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span><br>hp = json.load(<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;hp.json&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>))<br><br>model = nn.Sequential(<br>    nn.Linear(in_dim, hidden_dim),<br>    nn.GELU(),<br>    nn.Dropout(hp[<span class="hljs-string">&quot;dropout&quot;</span>]),<br>    nn.Linear(hidden_dim, hidden_dim),<br>    nn.GELU(),<br>    nn.Dropout(hp[<span class="hljs-string">&quot;dropout&quot;</span>]),<br>    nn.Linear(hidden_dim, out_dim)<br>)<br>model.cuda()<br><br><span class="hljs-comment">######################################################</span><br>opt = Adam(model.parameters(), lr=hp[<span class="hljs-string">&quot;lr&quot;</span>] * hvd.size())<br>opt = hvd.DistributedOptimizer(opt, named_parameters=model.named_parameters(), op=hvd.Average)<br>hvd.broadcast_parameters(model.state_dict(), root_rank=<span class="hljs-number">0</span>)<br>hvd.broadcast_optimizer_state(opt, root_rank=<span class="hljs-number">0</span>)<br><span class="hljs-comment">######################################################</span><br><br>loss_fn = MSELoss()<br>epoch = <span class="hljs-number">20</span><br><br><span class="hljs-comment"># train and test</span><br>model.train()<br><span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>    <span class="hljs-comment">######################################################</span><br>    train_sampler.set_epoch(e)<br>    <span class="hljs-comment">######################################################</span><br>    avg_loss = []<br>    <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> train_dl:<br>        x = x.cuda()<br>        y = y.cuda()<br>        opt.zero_grad()<br>        p = model(x)<br>        loss = loss_fn(p, y)<br>        loss.backward()<br>        opt.step()<br>        avg_loss.append(loss.item())<br>    avg_loss = <span class="hljs-built_in">sum</span>(avg_loss) / <span class="hljs-built_in">len</span>(avg_loss)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Training. Epoch <span class="hljs-subst">&#123;e&#125;</span>, MSE loss: <span class="hljs-subst">&#123;avg_loss&#125;</span>, Worker: <span class="hljs-subst">&#123;hvd.rank()&#125;</span>&#x27;</span>)<br><br>model.<span class="hljs-built_in">eval</span>()<br><span class="hljs-keyword">with</span> torch.no_grad():<br>    avg_loss = []<br>    <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> test_dl:<br>        x = x.cuda()<br>        y = y.cuda()<br>        p = model(x)<br>        loss = loss_fn(p, y)<br>        avg_loss.append(loss.item())<br>    avg_loss = <span class="hljs-built_in">sum</span>(avg_loss) / <span class="hljs-built_in">len</span>(avg_loss)<br>    <span class="hljs-comment">######################################################</span><br>    avg_loss = hvd.allreduce(torch.tensor(avg_loss)).item()<br>    <span class="hljs-comment">######################################################</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Testing. MSE loss: <span class="hljs-subst">&#123;avg_loss&#125;</span>, Worker: <span class="hljs-subst">&#123;hvd.rank()&#125;</span>&#x27;</span>)<br><br>s3_save_model(s3_client, model, <span class="hljs-string">&#x27;songwei&#x27;</span>, <span class="hljs-string">&#x27;simple_ml/model_save/1/model.pt&#x27;</span>)<br>s3_save_file(s3_client, <span class="hljs-string">&#x27;config.pbtxt&#x27;</span>, <span class="hljs-string">&#x27;songwei&#x27;</span>, <span class="hljs-string">&#x27;simple_ml/model_save/1/config.pbtxt&#x27;</span>)<br></code></pre></td></tr></table></figure></details><!-- empty line --><p>测试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">horovodrun -np 4 -H localhost:4 python main_with_horovod.py<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>uncategorized</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Triton Server——Model Serving</title>
    <link href="/2022/06/19/triton-server/"/>
    <url>/2022/06/19/triton-server/</url>
    
    <content type="html"><![CDATA[<p>Triton Server官方文档可以参考，<a href="https://github.com/triton-inference-server/server/blob/main/docs/quickstart.md">https://github.com/triton-inference-server/server/blob/main/docs/quickstart.md</a></p><p>其支持的模型种类，参考，<a href="https://github.com/triton-inference-server/server/blob/main/docs/model_repository.md#model-files">https://github.com/triton-inference-server/server/blob/main/docs/model_repository.md#model-files</a></p><p>triton要求使用torchscript（可以认为是tensorflow v1），而非eager，参考，<a href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html">https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html</a>，因此在存储模型的时候要注意一下</p><p>每个模型需要一个配置文件来描述其输入输出：配置文件config.pbtxt的参考文档，<a href="https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md">https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md</a></p><p>config.pbtxt的一个简单示例：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">name:</span> <span class="hljs-string">&quot;simple&quot;</span><br><span class="hljs-symbol">platform:</span> <span class="hljs-string">&quot;pytorch_libtorch&quot;</span><br><span class="hljs-symbol">max_batch_size:</span> <span class="hljs-number">4</span><br>input [<br>  <span class="hljs-punctuation">&#123;</span><br><span class="hljs-symbol">    name:</span> <span class="hljs-string">&quot;input&quot;</span><br><span class="hljs-symbol">    data_type:</span> TYPE_FP32<br><span class="hljs-symbol">    dims:</span> [ <span class="hljs-number">2</span> ]<br>  <span class="hljs-punctuation">&#125;</span><br>]<br>output [<br>  <span class="hljs-punctuation">&#123;</span><br><span class="hljs-symbol">    name:</span> <span class="hljs-string">&quot;OUTPUT__0&quot;</span><br><span class="hljs-symbol">    data_type:</span> TYPE_FP32<br><span class="hljs-symbol">    dims:</span> [ <span class="hljs-number">1</span> ]<br>  <span class="hljs-punctuation">&#125;</span><br>]<br></code></pre></td></tr></table></figure><p>模型结构假设为</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stylus">in_dim, hidden_dim, out_dim = <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span><br>hp = json<span class="hljs-selector-class">.load</span>(<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;hp.json&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>))<br><br>model = nn<span class="hljs-selector-class">.Sequential</span>(<br>    nn<span class="hljs-selector-class">.Linear</span>(in_dim, hidden_dim),<br>    nn<span class="hljs-selector-class">.GELU</span>(),<br>    nn<span class="hljs-selector-class">.Dropout</span>(hp<span class="hljs-selector-attr">[<span class="hljs-string">&quot;dropout&quot;</span>]</span>),<br>    nn<span class="hljs-selector-class">.Linear</span>(hidden_dim, hidden_dim),<br>    nn<span class="hljs-selector-class">.GELU</span>(),<br>    nn<span class="hljs-selector-class">.Dropout</span>(hp<span class="hljs-selector-attr">[<span class="hljs-string">&quot;dropout&quot;</span>]</span>),<br>    nn<span class="hljs-selector-class">.Linear</span>(hidden_dim, out_dim)<br>)<br></code></pre></td></tr></table></figure><p>训练好模型之后，通过python代码将模型与配置文件都存储到S3中</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs routeros">session = boto3.session.Session()<br><br>s3_client = session.client(<br>    <span class="hljs-attribute">service_name</span>=<span class="hljs-string">&#x27;s3&#x27;</span>,<br>    <span class="hljs-attribute">aws_access_key_id</span>=<span class="hljs-string">&#x27;XXX&#x27;</span>,<br>    <span class="hljs-attribute">aws_secret_access_key</span>=<span class="hljs-string">&#x27;XXX&#x27;</span>,<br>    <span class="hljs-attribute">endpoint_url</span>=<span class="hljs-string">&#x27;&lt;http://10.105.222.7:24850&gt;&#x27;</span>,<br>)<br><span class="hljs-built_in">print</span>(s3_client.list_buckets())<br><br>buffer = io.BytesIO()<br>model_scripted = script(model)<br>torch.jit.save(model_scripted, buffer)<br>s3_client.put_object(<span class="hljs-attribute">Bucket</span>=<span class="hljs-string">&quot;mlflow-artifact&quot;</span>, <span class="hljs-attribute">Key</span>=<span class="hljs-string">&#x27;manual/test/model_repository/simple/1/model.pt&#x27;</span>,<br>                     <span class="hljs-attribute">Body</span>=buffer.getvalue())<br>with open(<span class="hljs-string">&#x27;config.pbtxt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) as f:<br>    s3_client.put_object(<span class="hljs-attribute">Bucket</span>=<span class="hljs-string">&quot;mlflow-artifact&quot;</span>, <span class="hljs-attribute">Key</span>=<span class="hljs-string">&#x27;manual/test/model_repository/simple/config.pbtxt&#x27;</span>,<br>                         <span class="hljs-attribute">Body</span>=f.read())<br></code></pre></td></tr></table></figure><p>之后通过triton server连接S3访问模型文件夹，先试试docker</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk">docker run --rm -p <span class="hljs-number">8000</span>:<span class="hljs-number">8000</span> -p <span class="hljs-number">8001</span>:<span class="hljs-number">8001</span> -p <span class="hljs-number">8002</span>:<span class="hljs-number">8002</span> \\<br>-e AWS_ACCESS_KEY_ID=XXX \\<br>-e AWS_SECRET_ACCESS_KEY=XXX \\<br>nvcr.io<span class="hljs-regexp">/nvidia/</span>tritonserver:<span class="hljs-number">22.05</span>-py3 tritonserver \\<br>--model-repository=s3:<span class="hljs-regexp">//</span>http:<span class="hljs-regexp">//</span><span class="hljs-number">10.105</span>.<span class="hljs-number">222.7</span>:<span class="hljs-number">24850</span><span class="hljs-regexp">/mlflow-artifact/m</span>anual<span class="hljs-regexp">/test/m</span>odel_repository<br></code></pre></td></tr></table></figure><p>Docker上没问题后，使用seldon core部署triton server到k8s集群中：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> machinelearning.seldon.io/v1<br><span class="hljs-symbol">kind:</span> SeldonDeployment<br><span class="hljs-symbol">metadata:</span><br><span class="hljs-symbol">  name:</span> triton-simple-model<br><span class="hljs-symbol">  namespace:</span> seldon<br><span class="hljs-symbol">spec:</span><br><span class="hljs-symbol">  name:</span> triton-simple<br><span class="hljs-symbol">  predictors:</span><br>  - graph:<br><span class="hljs-symbol">      implementation:</span> TRITON_SERVER<br><span class="hljs-symbol">      modelUri:</span> s3:<span class="hljs-comment">//mlflow-artifact/manual/test/model_repository</span><br><span class="hljs-symbol">      name:</span> classifier<br><span class="hljs-symbol">    name:</span> default<br><span class="hljs-symbol">    replicas:</span> <span class="hljs-number">1</span><br><span class="hljs-meta"># 【这个很重要，必须要加，对应了values中的镜像】</span><br><span class="hljs-symbol">  protocol:</span> v2<br></code></pre></td></tr></table></figure><p>部署完成后，验证</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gradle">curl triton-simple-model-<span class="hljs-keyword">default</span>-classifier:<span class="hljs-number">9000</span><span class="hljs-regexp">/v2/m</span>odels<span class="hljs-regexp">/simple/</span>config<br></code></pre></td></tr></table></figure><p>输出为</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs elixir">&#123;<span class="hljs-string">&quot;name&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;simple&quot;</span></span>,<span class="hljs-string">&quot;platform&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;pytorch_libtorch&quot;</span></span>,<span class="hljs-string">&quot;backend&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;pytorch&quot;</span></span>,<span class="hljs-string">&quot;version_policy&quot;</span><span class="hljs-symbol">:</span>&#123;<span class="hljs-string">&quot;latest&quot;</span><span class="hljs-symbol">:</span>&#123;<span class="hljs-string">&quot;num_versions&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">1</span>&#125;&#125;,<span class="hljs-string">&quot;max_batch_size&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">4</span>,<span class="hljs-string">&quot;input&quot;</span><span class="hljs-symbol">:</span>[&#123;<span class="hljs-string">&quot;name&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;input&quot;</span></span>,<span class="hljs-string">&quot;data_type&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;TYPE_FP32&quot;</span></span>,<span class="hljs-string">&quot;format&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;FORMAT_NONE&quot;</span></span>,<span class="hljs-string">&quot;dims&quot;</span><span class="hljs-symbol">:</span>[<span class="hljs-number">2</span>],<span class="hljs-string">&quot;is_shape_tensor&quot;</span><span class="hljs-symbol">:false</span>,<span class="hljs-string">&quot;allow_ragged_batch&quot;</span><span class="hljs-symbol">:false</span>,<span class="hljs-string">&quot;optional&quot;</span><span class="hljs-symbol">:false</span>&#125;],<span class="hljs-string">&quot;output&quot;</span><span class="hljs-symbol">:</span>[&#123;<span class="hljs-string">&quot;name&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;OUTPUT__0&quot;</span></span>,<span class="hljs-string">&quot;data_type&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;TYPE_FP32&quot;</span></span>,<span class="hljs-string">&quot;dims&quot;</span><span class="hljs-symbol">:</span>[<span class="hljs-number">1</span>],<span class="hljs-string">&quot;label_filename&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;&quot;</span></span>,<span class="hljs-string">&quot;is_shape_tensor&quot;</span><span class="hljs-symbol">:false</span>&#125;],<span class="hljs-string">&quot;batch_input&quot;</span><span class="hljs-symbol">:[]</span>,<span class="hljs-string">&quot;batch_output&quot;</span><span class="hljs-symbol">:[]</span>,<span class="hljs-string">&quot;optimization&quot;</span><span class="hljs-symbol">:</span>&#123;<span class="hljs-string">&quot;priority&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;PRIORITY_DEFAULT&quot;</span></span>,<span class="hljs-string">&quot;input_pinned_memory&quot;</span><span class="hljs-symbol">:</span>&#123;<span class="hljs-string">&quot;enable&quot;</span><span class="hljs-symbol">:true</span>&#125;,<span class="hljs-string">&quot;output_pinned_memory&quot;</span><span class="hljs-symbol">:</span>&#123;<span class="hljs-string">&quot;enable&quot;</span><span class="hljs-symbol">:true</span>&#125;,<span class="hljs-string">&quot;gather_kernel_buffer_threshold&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">0</span>,<span class="hljs-string">&quot;eager_batching&quot;</span><span class="hljs-symbol">:false</span>&#125;,<span class="hljs-string">&quot;instance_group&quot;</span><span class="hljs-symbol">:</span>[&#123;<span class="hljs-string">&quot;name&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;simple&quot;</span></span>,<span class="hljs-string">&quot;kind&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;KIND_CPU&quot;</span></span>,<span class="hljs-string">&quot;count&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">1</span>,<span class="hljs-string">&quot;gpus&quot;</span><span class="hljs-symbol">:[]</span>,<span class="hljs-string">&quot;secondary_devices&quot;</span><span class="hljs-symbol">:[]</span>,<span class="hljs-string">&quot;profile&quot;</span><span class="hljs-symbol">:[]</span>,<span class="hljs-string">&quot;passive&quot;</span><span class="hljs-symbol">:false</span>,<span class="hljs-string">&quot;host_policy&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;&quot;</span></span>&#125;],<span class="hljs-string">&quot;default_model_filename&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;model.pt&quot;</span></span>,<span class="hljs-string">&quot;cc_model_filenames&quot;</span><span class="hljs-symbol">:</span>&#123;&#125;,<span class="hljs-string">&quot;metric_tags&quot;</span><span class="hljs-symbol">:</span>&#123;&#125;,<span class="hljs-string">&quot;parameters&quot;</span><span class="hljs-symbol">:</span>&#123;&#125;,<span class="hljs-string">&quot;model_warmup&quot;</span><span class="hljs-symbol">:[]</span>&#125;<br></code></pre></td></tr></table></figure><p>验证模型推断</p><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs coq">curl <span class="hljs-number">10.111</span><span class="hljs-number">.154</span><span class="hljs-number">.79</span>:<span class="hljs-number">9000</span>/v2/models/<span class="hljs-built_in">simple</span>/infer -d \\<br>&#x27;&#123;<span class="hljs-string">&quot;inputs&quot;</span>:[&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;input&quot;</span>,<span class="hljs-string">&quot;datatype&quot;</span>:<span class="hljs-string">&quot;FP32&quot;</span>,<span class="hljs-string">&quot;shape&quot;</span>:[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],<span class="hljs-string">&quot;data&quot;</span>:[[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]]&#125;]&#125;&#x27;<br></code></pre></td></tr></table></figure><p>模型输出为</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs prolog">&#123;<span class="hljs-string">&quot;model_name&quot;</span>:<span class="hljs-string">&quot;simple&quot;</span>,<span class="hljs-string">&quot;model_version&quot;</span>:<span class="hljs-string">&quot;1&quot;</span>,<span class="hljs-string">&quot;outputs&quot;</span>:[&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;OUTPUT__0&quot;</span>,<span class="hljs-string">&quot;datatype&quot;</span>:<span class="hljs-string">&quot;FP32&quot;</span>,<span class="hljs-string">&quot;shape&quot;</span>:[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],<span class="hljs-string">&quot;data&quot;</span>:[<span class="hljs-number">15.969637870788575</span>]&#125;]&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>uncategorized</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
